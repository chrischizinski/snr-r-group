<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on SNR R User Group</title>
    <link>/post/</link>
    <description>Recent content in Posts on SNR R User Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Age-cohort analysis</title>
      <link>/post/2018-02-23-agecohort/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-02-23-agecohort/</guid>
      <description>Working with Matt’s data # Download from google drive id &amp;lt;- &amp;quot;1wq8UNl921-NeCs0VCIhlGfbBK9ZxB3kL&amp;quot; huntData &amp;lt;- read_csv(sprintf(&amp;quot;https://docs.google.com/uc?id=%s&amp;amp;export=download&amp;quot;, id)) ## Parsed with column specification: ## cols( ## permitYear = col_integer(), ## permitType = col_character(), ## gender = col_character(), ## age = col_integer(), ## residency = col_character(), ## birthyear = col_integer() ## ) glimpse(huntData) ## Observations: 723,480 ## Variables: 6 ## $ permitYear &amp;lt;int&amp;gt; 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 200... ## $ permitType &amp;lt;chr&amp;gt; &amp;quot;Hunt Fish Combo&amp;quot;, &amp;quot;Hunt Fish Combo&amp;quot;, &amp;quot;Hunt Fish Co.</description>
    </item>
    
    <item>
      <title>Applied Multivariate: Identifying differences between groups</title>
      <link>/post/2017-11-17-differencesbetweengroups/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-17-differencesbetweengroups/</guid>
      <description>library(vegan) library(gridExtra) library(tidyverse) Linear discriminant analysis  Often we have groups that we have defined a priori before doing an analysis and we seek to understand what makes those groups different
 Linear discrimination analysis (LDA), similar to multinomial logistic regression, attempts to find linear combinations of variables that best separate groups when predicting two or more dependent variables, using continuous independent variables.
  Load the data We will use the data set morph_data.</description>
    </item>
    
    <item>
      <title>Latent variable analysis. Part 3</title>
      <link>/post/2017-11-09-latentvariables_3/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-09-latentvariables_3/</guid>
      <description>library(poLCA) # poLCA: Polytomous Variable Latent Class Analysis #library(lavaan) # Latent Variable Analysis #library(lcca) # Latent Class Causal Analysis library(BayesLCA) #Bayesian Latent Class Analysis library(gridExtra) library(tidyverse) Latent variable analysis Latent class analysis  Latent class analysis is another technique that is used to describe the latent groups in multivariate data Used on polytomous data (so good for analysis of survey data) Latent Class Analysis (LCA) is a statistical method for identifying unmeasured class membership among subjects using categorical and/or continuous observed variables.</description>
    </item>
    
    <item>
      <title>Latent variable analysis. Part 2</title>
      <link>/post/2017-10-20-latentvariables_2/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-10-20-latentvariables_2/</guid>
      <description>library(psych) library(GGally) library(ggrepel) library(gridExtra) library(polycor) library(poLCA) library(tidyverse) Latent variable analysis Factor analysis Exploratory factor analysis (continued) Load the data
goal_scale &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/goal_scale.csv&amp;quot;) ## Parsed with column specification: ## cols( ## ags1 = col_integer(), ## ags2 = col_integer(), ## ags3 = col_integer(), ## ags4 = col_integer(), ## ags5 = col_integer(), ## ags6 = col_integer(), ## ags7 = col_integer(), ## ags8 = col_integer(), ## ags9 = col_integer(), ## ags10 = col_integer(), ## ags11 = col_integer(), ## ags12 = col_integer() ## ) head(goal_scale) ## # A tibble: 6 x 12 ## ags1 ags2 ags3 ags4 ags5 ags6 ags7 ags8 ags9 ags10 ags11 ags12 ## &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; ## 1 6 6 6 7 7 5 7 6 6 7 6 6 ## 2 5 5 6 6 7 6 5 6 6 7 5 5 ## 3 5 6 2 3 7 6 7 2 4 2 2 7 ## 4 5 4 7 7 6 5 4 7 7 7 7 4 ## 5 5 4 5 5 6 5 5 4 5 5 3 3 ## 6 7 7 7 7 7 7 7 7 7 7 7 7 Fit an EFA models with factanal</description>
    </item>
    
    <item>
      <title>Latent variable analysis. Part 1</title>
      <link>/post/2017-10-13-latentvariables_1/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-10-13-latentvariables_1/</guid>
      <description>library(psych) Answers to the challenge library(tidyverse) ## ── Attaching packages ──── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## ✖ ggplot2::%+%() masks psych::%+%() ## ✖ ggplot2::alpha() masks psych::alpha(), scales::alpha() ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggdendro) library(NbClust) data(&amp;quot;USArrests&amp;quot;) USArrests %&amp;gt;% scale() -&amp;gt; arrest.</description>
    </item>
    
    <item>
      <title>Applied Multivariate:  Breaking multivariate data into groups. Part 2.</title>
      <link>/post/2017-10-06-clustering_2/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-10-06-clustering_2/</guid>
      <description>library(tidyverse) library(cluster) library(vegan) library(factoextra) library(fpc) library(RWeka) library(ggdendro) library(NbClust) First lets load the data from last week data(&amp;quot;USArrests&amp;quot;) Lets scale the data
USArrests %&amp;gt;% scale() -&amp;gt; arrest.scale head(arrest.scale) ## Murder Assault UrbanPop Rape ## Alabama 1.24256408 0.7828393 -0.5209066 -0.003416473 ## Alaska 0.50786248 1.1068225 -1.2117642 2.484202941 ## Arizona 0.07163341 1.4788032 0.9989801 1.042878388 ## Arkansas 0.23234938 0.2308680 -1.0735927 -0.184916602 ## California 0.27826823 1.2628144 1.7589234 2.067820292 ## Colorado 0.02571456 0.3988593 0.8608085 1.864967207 lets convert this to a distance matrix using the factoextra::get_dist() function.</description>
    </item>
    
    <item>
      <title>Applied Multivariate:  Breaking multivariate data into groups. Part 1.</title>
      <link>/post/2017-09-29-clustering_1/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-29-clustering_1/</guid>
      <description>library(tidyverse) library(vegan) library(cluster) library(factoextra) library(fpc) Cluster analysis This is a broad topic and could probably cover most of a semester, if you want more in depth start by looking at:

The background  Cluster analysis is a broad group of multivariate techniques to identify homogenous groups  maximizes between group variation and minimizing within group variation outcome: reduction of observations into fewer groups often used in data mining or exploratory approaches works best when there are inherent discontinuities in the data  if the data is continuous, ordination techniques may be preferred ordination may force groups that do not exist   Occurs in two basic steps: measure of similarity betewen observations is specified Using this distance (and a clustering rule) observations are grouped based on either a hierarchical or partitioning technique Once a new cluster is formed, distances between clusters are based on single linkage (minimum distance), complete linkage (maximum method), or average linkage  Hiercharchical techniques are useful because they can reveal relationships in a nested fashion (i.</description>
    </item>
    
    <item>
      <title>Applied Multivarite:  Dissimilarity and Distance measures</title>
      <link>/post/2017-09-29-dissimilarites/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-29-dissimilarites/</guid>
      <description>This class covers an introduction to dissimilarity and distance measures. Detailed notes from previous semesters can be found here - 2016-08-12-SimilarityDistance.
 R notebook Rmarkdown file  Load the libraries we will use today
library(tidyverse) library(vegan) library(cluster) library(RColorBrewer) Similarity and distances To illustrate the concept of similarity and distance, lets envison a data matrix with 4 sites and 2 species
hyp_data &amp;lt;- matrix(c(1,9,1,8,6,6,9,1), byrow = TRUE, ncol = 2) colnames(hyp_data)&amp;lt;-c(&amp;quot;SpeciesA&amp;quot;, &amp;quot;SpeciesB&amp;quot;) hyp_data ## SpeciesA SpeciesB ## [1,] 1 9 ## [2,] 1 8 ## [3,] 6 6 ## [4,] 9 1 Lets plot these in 2 dimensions to show the relationships</description>
    </item>
    
    <item>
      <title>Intro to multivariate statistics</title>
      <link>/post/2017-09-15-introtomultivariate/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-15-introtomultivariate/</guid>
      <description>This class covers an introduction to multivariate statistics and data transformations and standardizations. Detailed notes from previous semesters can be found
 2016-08-10-Data-Transformations.
 Introduction presentation
 R notebook Rmarkdown file
  Difference between Transformations and Standardizations  Transformations are applied to each element in a matrix Standardization adjust elements in a matrix by a row or column statistic  Create some data rawdata &amp;lt;- matrix(c(1,1,1,3,3,1, 2,2,4,6,6,0, 10,10,20,30,30,0, 3,3,2,1,1,0, 0,0,0,20,0,0), ncol = 6, byrow = TRUE) colnames(rawdata) &amp;lt;- paste(&amp;quot;species&amp;quot;,toupper(letters[1:6]), sep = &amp;quot;_&amp;quot;) rawdata ## species_A species_B species_C species_D species_E species_F ## [1,] 1 1 1 3 3 1 ## [2,] 2 2 4 6 6 0 ## [3,] 10 10 20 30 30 0 ## [4,] 3 3 2 1 1 0 ## [5,] 0 0 0 20 0 0  Calculating row and column statistics Rows # Row sums rowSums(rawdata) ## [1] 10 20 100 10 20 apply(rawdata, 1, sum) ## [1] 10 20 100 10 20 # Max values apply(rawdata, 1, max) ## [1] 3 6 30 3 20  Columns # Sums apply(rawdata, 2, sum) ## species_A species_B species_C species_D species_E species_F ## 16 16 27 60 40 1 colSums(rawdata) ## species_A species_B species_C species_D species_E species_F ## 16 16 27 60 40 1 # Max apply(rawdata, 2, max) ## species_A species_B species_C species_D species_E species_F ## 10 10 20 30 30 1   Monotonic transformations Log transformations  Useful for when you have a wide spread in data values</description>
    </item>
    
    <item>
      <title>Intro to ggplot2</title>
      <link>/post/2017-09-08-introtoggplot2/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-08-introtoggplot2/</guid>
      <description>This class covers an introduction to graphing using the functions from ggplot2. Similar to the introduction with tidyverse This is only meant to serve as a brief introduction and we will keep developing these skills throughout the semester. I have similar lessons graphing from a previous semester 2016-10-03-BasicPlots and information on themes, facetting, and saving plots 2016-10-05-Themes_Facets.
R script from class
Answers to last week challenges
Weekly challenge Take home challenges 1.</description>
    </item>
    
    <item>
      <title>Intro to Data Wrangling</title>
      <link>/post/2017-09-01-introtodatawrangling/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-09-01-introtodatawrangling/</guid>
      <description>This class covers an introduction to wrangling and summarizing data using the functions from tidyr and dplyr. This is only meant to serve as a brief introduction, as we will keep developing these skills throughout the semester. I have a similar lesson on data wrangling from a previous semester 2016-09-23-Wrangling and information on joins 2016-09-29-Joining_Data_Sets.
R script from class
Answers to last week challenges
Weekly challenge The data Work with the ecology data set from datacarpentry.</description>
    </item>
    
    <item>
      <title>Getting to know R -- yet again</title>
      <link>/post/2017-08-25-introtor/</link>
      <pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-08-25-introtor/</guid>
      <description>Welcome back to another semester of our UseR.
R script from class
Powerpoint from class
Weekly challenge The data Work with the ecology data set from datacarpentry. An explanation of the dataset can be found here
library(tidyverse) mydata &amp;lt;- read_csv(&amp;quot;https://ndownloader.figshare.com/files/2292169&amp;quot;) glimpse(mydata) ## Observations: 34,786 ## Variables: 13 ## $ record_id &amp;lt;int&amp;gt; 1, 72, 224, 266, 349, 363, 435, 506, 588, 661,... ## $ month &amp;lt;int&amp;gt; 7, 8, 9, 10, 11, 11, 12, 1, 2, 3, 4, 5, 6, 8, .</description>
    </item>
    
    <item>
      <title>Ecological Detective -- Maximum likelihood</title>
      <link>/post/2017-04-28-likelihood/</link>
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-04-28-likelihood/</guid>
      <description>library(tidyverse) library(broom) Sources of the notes for this lecture are from Ecological Detective (Chapter 4).
Overview  Sum of squares can be used to find the best fit of a model to the data under minimal assumptions about the sources of uncertainty There are many cases in which the form of the probability distributions of the uncertain terms can be justified likelihood methods allow us to calculate confidence bounds on parameters and to test hypotheses in the traditional manner forms the foundation for Bayesian analysis  The probability of observing data \(Y_i\) given a parameter \(p\) is \( Pr\{Y_i|p\}\).</description>
    </item>
    
    <item>
      <title>Ecological Detective -- The Confrontation:  Sum of squares</title>
      <link>/post/2017-04-14-sumofsquares/</link>
      <pubDate>Fri, 14 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-04-14-sumofsquares/</guid>
      <description>library(tidyverse) library(broom) Sources of the notes for this lecture are from Ecological Detective (Chapter 5).
 Simplest technique for the confrontation between models and data is sum of squares
It is simple and makes few assumptions Long and successful history in science Computers can do remarkable calcualations associated with sum of squares   Basic method Consider a simple model: \[ Y_i = A + BX_i = CX_i^2 + W_i\] where \(W_i\) is process uncertainty, and A, B, and C are parameters.</description>
    </item>
    
    <item>
      <title>Incidental catch in fisheries:  seabirds in the New Zealand squid trawl fisheries</title>
      <link>/post/2017-03-30-incidentalcatch/</link>
      <pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-03-30-incidentalcatch/</guid>
      <description>library(tidyverse) library(broom) Sources of the notes for this lecture are from Ecological Detective (Chapter 4).
Motivation Squid trawl
  Non-target species are often caught during fishing operations Observer programs are used to monitor this incidental catch Understanding of the coverage of the program and how to interpret the data is needed  hauls = c(807, 37, 27, 8, 4, 4, 1, 3, 1, 0, 0, 2, 1, 1, 0, 0, 0, 1) # from table 4.</description>
    </item>
    
    <item>
      <title>Ecological Detective -- Probability and probability models, Part 1</title>
      <link>/post/2017-03-10-ecologicaldetective5/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-03-10-ecologicaldetective5/</guid>
      <description>library(tidyverse) Classical Probability Sources of the notes for this lecture are a combination of Aho(2013) (Chapters 2 and 3) and Ecological Detective (Chapters 3 and 4).
 As we become familiar with the behavior of random variables, we may become aware of probabilistic patterns  Disjoint If two events can not occur simultaneously, then we call them mutually exclusive or disoint
If for two outcomes A and B, we wanted to know the probability of the event A or B (expressed as: \(P( A B) = P(A) + P(B) \))</description>
    </item>
    
    <item>
      <title>Ecological Detective -- Relationships and probability.  Part 2</title>
      <link>/post/2017-03-03-sixsideddierolls/</link>
      <pubDate>Fri, 03 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-03-03-sixsideddierolls/</guid>
      <description>What about for a six sided die?
Let’s take the coin-flipping example from last week and run a similar test on a six sided dice. Lets run a simulation from 1 to 5000 dice rolls and calculated the proportion of each number. For example, a 1 will be a single roll and we will count the number of each number (should be all zeros except for one value for one of the numbers).</description>
    </item>
    
    <item>
      <title>Ecological Detective -- Relationships and probability</title>
      <link>/post/2017-02-23-ecologicaldetective3/</link>
      <pubDate>Thu, 23 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-02-23-ecologicaldetective3/</guid>
      <description>Exploring the relationship between two variables First lets bring in the data from the previous lesson
library(tidyverse) library(broom) fish_data &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/chrischizinski/MWFWC_FishR/master/CourseMaterial/data/wrkshp_data.csv&amp;quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## WaterbodyCode = col_integer(), ## Area = col_integer(), ## MethodCode = col_integer(), ## surveydate = col_datetime(format = &amp;quot;&amp;quot;), ## Station = col_integer(), ## Effort = col_integer(), ## SpeciesCode = col_integer(), ## LengthGroup = col_integer(), ## FishCount = col_integer(), ## FishLength = col_integer(), ## FishWeight = col_integer(), ## Age = col_integer(), ## Edge = col_integer(), ## Annulus1 = col_integer(), ## Annulus2 = col_integer(), ## Annulus3 = col_integer(), ## Annulus4 = col_integer(), ## Annulus5 = col_integer(), ## Annulus6 = col_integer(), ## Annulus7 = col_integer() ## # .</description>
    </item>
    
    <item>
      <title>Ecological Detective -- Know your data</title>
      <link>/post/2017-02-10-ecologicaldetective2/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-02-10-ecologicaldetective2/</guid>
      <description>Recreating Figure 2.1 in Ecological Detective There are many different hypotheses that can explain the basic relationship between two variables. Figure 2.1 in the Ecologial Detective suggest 4 possible models. The models have no parameter values. Try to iteratively find the parameter values to get your figure to look like the one in Figure 2.1.
S &amp;lt;- seq(1,15, by = 1) # 1:15 Null_hypothesis&amp;lt;- 2.5 our_data &amp;lt;- data.frame(S = S, Null = Null_hypothesis) our_data$Model_A &amp;lt;- 0.</description>
    </item>
    
    <item>
      <title>Ecological Detective #1</title>
      <link>/post/2017-02-03-ecologicaldetective1/</link>
      <pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-02-03-ecologicaldetective1/</guid>
      <description>Ecological Detective Preface: Beyond the Null hypothesis  Tools of an Ecological Detective  Classic Hypothesis Testing  M1: Boom and bust (complete extinction between colonizations) hypothesis M2: Constant prevalance of population but only is detected when certain conditions arise
 H0: Model M1 is true HA: Some other model is true
  Outcome:
M2 is rejected but M1 is not M1 is rejected but M2 is not M1 and M2 are rejected M1 and M2 are not rejected  What if 1 and 2 happen?</description>
    </item>
    
    <item>
      <title>Basic RMarkdown</title>
      <link>/post/2017-01-20-rmarkdowntutorial/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-01-20-rmarkdowntutorial/</guid>
      <description>This is just a gentle introduction to what you can do with Rmarkdown. There are lots of tutorials out there to help you further (Software carpentry, Data carpentry). There is also a lot more you can do with it than I have shown here. For example, you can alter the look of your html using CSS. In addition, given the limitation of the computers in the lab (i.e., no MikeTEX), we are going to limit ourselves to html this lesson.</description>
    </item>
    
    <item>
      <title>Getting to know R -- Again</title>
      <link>/post/2017-01-13-gettingtoknowragain/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-01-13-gettingtoknowragain/</guid>
      <description>Getting to know R again (with some new stuff thrown in) Using data from Google sheets (kind of) Google Forms is a quick and cheap way to put together an online survey. I asked you all to do the survey through Google Forms so that we can run some of the analysis with your responses. The survey responses were scrubbed of any individual information.
Getting data straight into used to be a bit more complicated but thanks to the googlesheets package it is a lot easier.</description>
    </item>
    
    <item>
      <title>Multiple factor ANOVA</title>
      <link>/post/2016-12-16-anova_multifactor/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-12-16-anova_multifactor/</guid>
      <description>The RMarkdown file for this lesson can be found here.
This lesson will follow Chapter 9, 10, and in Quinn and Keough (2002).
Load the packages we will be using in this lesson
library(tidyverse) library(broom) library(multcomp) # install.packages(&amp;#39;afex&amp;#39;) # library(afex) Nested designs Common extension of the single factor design is the nested design - additional factors are included that are nested within the main factor of interest - characteristic feature that distinguishes from other multifactor designs is that the categories of the nested factor(s) within each level of the main factor are different.</description>
    </item>
    
    <item>
      <title>ANOVA</title>
      <link>/post/2016-12-09-anova_1factor/</link>
      <pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-12-09-anova_1factor/</guid>
      <description>The RMarkdown file for this lesson can be found here.
This lesson will follow Chapter 8 in Quinn and Keough (2002).
Load the packages we will be using in this lesson
library(tidyverse) library(broom) library(lme4) library(multcomp) Comparing groups or treatments  Analysis of variance (ANOVA) is a statistical technique to partition and analyze the variation of a continuous response variable Previously we used ANOVA to partition the variation in a response variable into that explained by the linear regression with one or more continuous predictor variables and that unexplained by the regression model The statistical distinction between “classical regression” and “classical ANOVA” is artificial, which is why we can use the lm() with anova() or the aov function in R Two prime reasons to use classical ANOVA: examine the relative contribution of sources of variation to the total amount of the variability in the response variable test the null hypothesis (H0) that population group or treatment means are equal   Single factor  A single factor or one way design = single factor or predictor  factor can comprise several levels completely randomized (CR) designs (no restriction on the random allocation of experimental or sampling units to factor levels)   Types of predictors  Two types of factors  Fixed - all the levels of the factor that are of interest are included in the analysis  cannot extrapolate beyond these levels, repeat experiment keep same levels called: fixed effect models or Model 1 ANOVAs conclusions for a fixed factor are restricted to those specific groups we used in the experiment or sampling program  Random - we are only using a random selection of all the possible levels of the factor  usually make inferences about all the possible groups from our sample of groups called: random effect models or Model 2 ANOVAs analogous to Model 2 regression draw conclusions about the population of groups from which we have randomly chosen a subset (like site or time)    Lets begin exploring this in R, using the medley data</description>
    </item>
    
    <item>
      <title>Multiple Regression</title>
      <link>/post/2016-11-11-multipleregression/</link>
      <pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-11-11-multipleregression/</guid>
      <description>The RMarkdown file for this lesson can be found here.
This lesson will follow Chapter 6 in Quinn and Keough (2002).
Load the packages we will be using in this lesson
library(RCurl) library(tidyverse) library(broom) library(GGally) library(devtools) library(gridExtra) source_url(&amp;#39;https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/R/snr_r_group_functions.R&amp;#39;) Multiple Linear regression analysis Our previous lesson was based on regression models with a single predictor and single response variable. We can expand on these by increasing the number of predictor variables, which are called are multiple linear regression models.</description>
    </item>
    
    <item>
      <title>Simple Regression</title>
      <link>/post/2016-10-28-simpleregression/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-28-simpleregression/</guid>
      <description>The RMarkdown file for this lesson can be found here.
This lesson will follow Chapter 5 in Quinn and Keough (2002).
Load the packages we will be using in this lesson
library(MASS) library(car) library(RCurl) library(mgcv) library(tidyverse) library(broom) library(Rfit) library(mgcv) library(gtable) library(lmodel2) Linear regression analysis Statistical models that assume a linear relationship between a single, continuous (usually) predictor value are simple linear regression models.
These models have three primary purposes:
 describe a linear relationship between \( Y \) and \( X \) determine the amount of variation (explained) in \( Y \) with \( X \) and the amount of variation unexplained predict values of \( Y \) from \( X \)  Simple bivariate linear regression Linear model for regression Consider you have a set of observations (\( i = 1 :n \) ), where the each observation was chosen based on its \( X \) value and its \( Y \) value for each observation is sampled from a population of possible \( Y \) values.</description>
    </item>
    
    <item>
      <title>Correlation</title>
      <link>/post/2016-10-20-correlation/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-20-correlation/</guid>
      <description>library(tidyverse) ## ── Attaching packages ──── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(gridExtra) ## ## Attaching package: &amp;#39;gridExtra&amp;#39; ## The following object is masked from &amp;#39;package:dplyr&amp;#39;: ## ## combine library(RCurl) ## Loading required package: bitops ## ## Attaching package: &amp;#39;RCurl&amp;#39; ## The following object is masked from &amp;#39;package:tidyr&amp;#39;: ## ## complete library(MASS) ## ## Attaching package: &amp;#39;MASS&amp;#39; ## The following object is masked from &amp;#39;package:dplyr&amp;#39;: ## ## select The RMarkdown file for this lesson can be found here</description>
    </item>
    
    <item>
      <title>Hypothesis Testing</title>
      <link>/post/2016-10-13-hypothesistesting/</link>
      <pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-13-hypothesistesting/</guid>
      <description>Statistical hypothesis testing In the biological sciences, among other sciences, it is not often enough to just collect information on the central tendency of a population or parameter. We often want to compare estimates among populations or against environmental variables. Perhaps not surprisingly, there is still controversy on how this should be approached and the philosophies behind the approach. Chapter two in the the Ecological Detective: Confronting Models with Data has a great synopsis of these.</description>
    </item>
    
    <item>
      <title>Estimation of parameters</title>
      <link>/post/2016-10-07-restimation/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-07-restimation/</guid>
      <description>Samples and populations Biologists want to make inferences about a population based on subsamples of that population.
 collections of the population are the sample number of observations is the sample size  Basic method of sampling is simple random sampling (all observations have the same probability of being sampled)
 rarely does this happen (Why is this a concern?)  Random sampling is important because we want to use sample statistics to estimate the population parameters.</description>
    </item>
    
    <item>
      <title>Estimation of parameters.  Part 2</title>
      <link>/post/2016-10-07-restimation2/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-07-restimation2/</guid>
      <description>Like determining population parameters , we often want to calculate the parameters in our regression models. There are two basic procedures that are often used to determine those. These sections are only meant to be illustrative and not comprehensive into the topic.
I based these illustrations heavily on the material from the Ecological Detective: Confronting Models with Data. I highly encourage you to read this book and follow along with the psuedo code in the book.</description>
    </item>
    
    <item>
      <title>Graphing themes and facets</title>
      <link>/post/2016-10-05-themes_facets/</link>
      <pubDate>Wed, 05 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-05-themes_facets/</guid>
      <description>Themes One of my favorite aspects of ggplot2 is the use of themes. Not including a theme, the default ggplots theme is the theme_grey which has a dark grey background with white grid lines. See the example below
library(tidyverse) ## ── Attaching packages ──── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.</description>
    </item>
    
    <item>
      <title>Web Graphics</title>
      <link>/post/2016-10-04-webgraphics/</link>
      <pubDate>Tue, 04 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-10-04-webgraphics/</guid>
      <description>var lodash = _.noConflict();  Plots for the web There has been increasing need and desirability to produce interactive graphics online. News outlets, like the The Economist, New York Times, Vox, 538, Pew, and Quartz, routinely use interactive displays.
There are two packages (among several) that allow us to create interactive graphics. The first is ggivis, which is based on ggplot2 and the other is googlevis.</description>
    </item>
    
    <item>
      <title>Plotting</title>
      <link>/post/2016-09-30-basicplots/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-09-30-basicplots/</guid>
      <description>Basic plotting in ggplot ggplot is a package that has truly upped the level of producing quality graphics using R. The “g g” in ggplot refers to the grammar of graphics. There has been a lot of development of the theory in what makes a good plot and I encourage you to read more on the subject.
From the ggplot2 website
 ggplot2 is a plotting system for R, based on the grammar of graphics, which tries to take the good parts of base and lattice graphics and none of the bad parts.</description>
    </item>
    
    <item>
      <title>Joining data</title>
      <link>/post/2016-09-29-joining_data_sets/</link>
      <pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-09-29-joining_data_sets/</guid>
      <description>Very often when we are working with datasets, particularly databases, we often want to combine multiple datasets. Traditionally, in R, this was done with the function merge(). Since the development of the tidyverse there has been improvements on the types and speed of joins. The joins available in the dplyr() package follow those in SQL type joins.
We will explore these types of joins using datasets from the our github repository</description>
    </item>
    
    <item>
      <title>Data wrangling</title>
      <link>/post/2016-09-23-wrangling/</link>
      <pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-09-23-wrangling/</guid>
      <description>All the data that we use is in our repository.
Now that you have your data in R, no matter the process you took to get it there, you will undoubtedly need to manipulate the data. Manipulation (i.e. tidying) will involve getting your data in a format and “looking” the way you want so you can analyze the data. This does not sound like a big process but until a few years ago it was.</description>
    </item>
    
    <item>
      <title>R Data Structures</title>
      <link>/post/2016-09-02-datastructures/</link>
      <pubDate>Mon, 19 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-09-02-datastructures/</guid>
      <description>Working with data in R There are lots of great references out there to help orient you with R and R data structures. One of the best is the section on data structures in the Advanced R book by Hadley Wickham. Hadley provides numerous details on differences among the structures and a lot of the nitty gritty on those structures. This lesson is suppose to provide a “working knowledge” of data structures in R, but I strongly encourage you to dive into more.</description>
    </item>
    
    <item>
      <title>Inputing data</title>
      <link>/post/2016-09-09-datainput/</link>
      <pubDate>Fri, 09 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-09-09-datainput/</guid>
      <description>All the data that we use (and will be used in this course) are available from here. I have also placed a copy of this data in our repository.
Getting data into R There are a lot of ways of getting data into R and this can add to a lot of confusion for R newbies trying to get started in R. We have already shown that there are ways to manually enter data in the previous lesson.</description>
    </item>
    
    <item>
      <title>Getting to know R</title>
      <link>/post/2016-08-26-rbasics/</link>
      <pubDate>Fri, 26 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-08-26-rbasics/</guid>
      <description>This is the second lesson in “Getting to know R.” In our first lesson, we went over the very basics of R including installation, RStudio, and some of the basics of learning and getting help in R. The next sections we will begin to layout some of the “vocabulary” of R. For those that have used R before and know that R is a lot more than a letter in the alphabet, most of this will be a review.</description>
    </item>
    
    <item>
      <title>Similarities and Dissimilarities</title>
      <link>/post/2016-08-12-similaritydistance/</link>
      <pubDate>Fri, 12 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016-08-12-similaritydistance/</guid>
      <description>Background The backbone to multivariate analysis is the two-way data matrix. In an ecological sense, we can imagine that each column in the dataset is a species or environmental factor and each row is a site. In a human dimensions sense, we can imagine a similar dataset where the columns are are responses to a question and each row is an individual.
This dataset can be be represented in an n-dimensional space based on its values within this data set.</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.
You can embed an R code chunk like this:
summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/answer1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/answer1/</guid>
      <description>library(RCurl) ## Loading required package: bitops library(tidyverse) ## ── Attaching packages ──── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ tidyr::complete() masks RCurl::complete() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() fao_data&amp;lt;-read_csv(getURL(&amp;quot;https://raw.</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/answer2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/answer2/</guid>
      <description>library(tidyverse) ## ── Attaching packages ──── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(RCurl) ## Loading required package: bitops ## ## Attaching package: &amp;#39;RCurl&amp;#39; ## The following object is masked from &amp;#39;package:tidyr&amp;#39;: ## ## complete lovett &amp;lt;- read_csv(getURL(&amp;quot;https://raw.</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/answer4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/answer4/</guid>
      <description>library(tidyverse) ## ── Attaching packages ──── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(RCurl) ## Loading required package: bitops ## ## Attaching package: &amp;#39;RCurl&amp;#39; ## The following object is masked from &amp;#39;package:tidyr&amp;#39;: ## ## complete ward&amp;lt;-read_csv(getURL(&amp;quot;https://raw.</description>
    </item>
    
    <item>
      <title>Ecological Detective - Probability and probability models. Part 2</title>
      <link>/post/2017-03-17-ecologicaldetective6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-03-17-ecologicaldetective6/</guid>
      <description>library(tidyverse) library(broom) Sources of the notes for this lecture are a combination of Aho(2013) (Chapters 2 and 3) and Ecological Detective (Chapters 3 and 4).
Common distributions Discrete Negative binomial distribution  Negative binomial gives the probability that x independent Bernoulli failures will occur prior to obtaining the rth success  two parameters: r is the number of successes and \( \pi \) is the probability of an individual Bernoulli success   First form:</description>
    </item>
    
    <item>
      <title>answer3</title>
      <link>/post/answer3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/answer3/</guid>
      <description># Challenge 1 set.seed(456789) fake_data2&amp;lt;-data.frame(x1 = rpois(50,5), x2 = rpois(50,2)) fake_data2$y &amp;lt;- fake_data2$x1 *2 + rnorm(50, sd = 2) + fake_data2$x2 *-4 + rnorm(50, sd = 2) start.val1&amp;lt;- -4 max.val1 &amp;lt;- 6 start.val2&amp;lt;- -8 max.val2&amp;lt;- 0 poss.vals.x1 &amp;lt;- seq(start.val1,max.val1, by = 0.05) poss.vals.x2 &amp;lt;- seq(start.val2,max.val2, by = 0.05) SS_stor2 &amp;lt;- expand.grid(x1 =poss.vals.x1, x2 = poss.vals.x2, SS = NA ) for(i in 1:nrow(SS_stor2)){ pred.vals2 &amp;lt;- fake_data2$x1 *SS_stor2$x1[i] + fake_data2$x2 *SS_stor2$x2[i] SS_stor2$SS[i] &amp;lt;- sum((fake_data2$y - pred.</description>
    </item>
    
  </channel>
</rss>