<!DOCTYPE html>
<html lang="en-us">
    <head>
         
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Applied Multivariate: Identifying differences between groups</title>
        <style>

    html body {
        font-family: 'Roboto', sans-serif;
        background-color: #FEFDFA;
    }

    :root {
        --accent: #D00000;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/hybrid.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/R.min.js"></script> 

    <script>hljs.initHighlightingOnLoad();</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.30.2" />
        
        
        
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-48496439-3"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-48496439-3');
        </script>
        
    </head>

    
    
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <body>
         
        <nav class="navbar navbar-default navbar-fixed-top">

            <div class="container">

                <div class="navbar-header">

                    <a class="navbar-brand visible-xs" href="#">Applied Multivariate: Identifying differences between groups</a>

                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>

                </div>

                <div class="collapse navbar-collapse">

                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/post/">Posts</a></li>
                            
                                <li><a href="/resources/">Resources</a></li>
                            
                        </ul>
                    

                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:cchizinski2@unl.edu"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/chrischizinski/"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://twitter.com/chrischizinski/"><i class="fa fa-twitter"></i></a></li>
                            
                        </ul>
                    

                </div>

            </div>

        </nav>


<main>

    <div class="item">

    
    
    

    
    

    <h4><a href="/post/2017-11-17-differencesbetweengroups/">Applied Multivariate: Identifying differences between groups</a></h4>
    <h5>November 17, 2017</h5>
    
    <a href="/tags/applied-multivariate"><kbd class="item-tag">Applied Multivariate</kbd></a>
    

</div>


    <br> <div class="text-justify"><pre class="r"><code>library(vegan) 
library(gridExtra)
library(tidyverse)</code></pre>
<div id="linear-discriminant-analysis" class="section level2">
<h2>Linear discriminant analysis</h2>
<ul>
<li><p>Often we have groups that we have defined <em>a priori</em> before doing an analysis and we seek to understand what makes those groups different</p></li>
<li><p>Linear discrimination analysis (LDA), similar to multinomial logistic regression, attempts to find linear combinations of variables that best separate groups when predicting two or more dependent variables, using continuous independent variables.</p></li>
</ul>
<div id="load-the-data" class="section level3">
<h3>Load the data</h3>
<p>We will use the data set <code>morph_data.csv</code> on <a href="">github</a>. The data consisted of 8 measurements in all 5
larval stages and the adults of the 6 species of the waterstrider
genus Limnoporus (Insecta: Heteroptera: Gerridae). Data was originally sourced from <a href="http://life.bio.sunysb.edu/morph/data/datasets.html">morphometry datasets</a>.</p>
<p>The variables are: Species, stage (1-5 for larvae, 6 for adults), sex (m male, f female, u undetermined), antseg1 - antseg4 (lengths of 1st to 4th antennal segments), midfem, midtib, hindfem, hindtib (lenghts of middle and hind femora and tibiae).
The values are raw measurements in millimeters.</p>
<pre class="r"><code>morph_data &lt;-read_csv(&quot;https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/morph_data.csv&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   Species = col_character(),
##   stage = col_integer(),
##   sex = col_character(),
##   antseg1 = col_double(),
##   antseg2 = col_double(),
##   antseg3 = col_double(),
##   antseg4 = col_double(),
##   midfem = col_double(),
##   midtib = col_double(),
##   hindfem = col_double(),
##   hindtib = col_double()
## )</code></pre>
<pre class="r"><code>glimpse(morph_data)</code></pre>
<pre><code>## Observations: 541
## Variables: 11
## $ Species &lt;chr&gt; &quot;canali&quot;, &quot;canali&quot;, &quot;canali&quot;, &quot;canali&quot;, &quot;canali&quot;, &quot;can...
## $ stage   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...
## $ sex     &lt;chr&gt; &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;, &quot;u&quot;,...
## $ antseg1 &lt;dbl&gt; 0.17, 0.20, 0.18, 0.16, 0.13, 0.18, 0.18, 0.18, 0.20, ...
## $ antseg2 &lt;dbl&gt; 0.15, 0.14, 0.14, 0.13, 0.13, 0.16, 0.12, 0.15, 0.15, ...
## $ antseg3 &lt;dbl&gt; 0.19, 0.18, 0.17, 0.14, 0.16, 0.18, 0.16, 0.16, 0.19, ...
## $ antseg4 &lt;dbl&gt; 0.72, 0.74, 0.62, 0.55, 0.56, 0.52, 0.72, 0.60, 0.67, ...
## $ midfem  &lt;dbl&gt; 0.70, 0.76, 0.76, 0.70, 0.72, 0.76, 0.74, 0.76, 0.78, ...
## $ midtib  &lt;dbl&gt; 0.80, 0.92, 0.94, 0.92, 0.92, 0.97, 0.91, 0.92, 0.95, ...
## $ hindfem &lt;dbl&gt; 0.43, 0.68, 0.79, 0.74, 0.68, 0.68, 0.70, 0.70, 0.78, ...
## $ hindtib &lt;dbl&gt; 0.72, 0.42, 0.40, 0.42, 0.38, 0.45, 0.42, 0.43, 0.44, ...</code></pre>
</div>
<div id="manipulate-the-data" class="section level3">
<h3>Manipulate the data</h3>
<p>For the purpose of this excercise lets limit the data to one species and only the measurements of the antennae.</p>
<pre class="r"><code>morph_data %&gt;% 
  select(Species:antseg4, -sex) %&gt;% 
  filter(Species == &quot;canali&quot;) %&gt;% 
  mutate_at(vars(antseg1:antseg4), scale)-&gt; canal_data

head(canal_data)</code></pre>
<pre><code>## # A tibble: 6 x 6
##   Species stage antseg1 antseg2 antseg3 antseg4
##   &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 canali      1   -1.31   -1.33   -1.38   -1.27
## 2 canali      1   -1.26   -1.36   -1.42   -1.22
## 3 canali      1   -1.30   -1.36   -1.45   -1.51
## 4 canali      1   -1.33   -1.39   -1.55   -1.68
## 5 canali      1   -1.39   -1.39   -1.48   -1.65
## 6 canali      1   -1.30   -1.31   -1.42   -1.75</code></pre>
</div>
<div id="conduct-the-linear-discriminant-analysis-using-masslda" class="section level3">
<h3>Conduct the linear discriminant analysis using <code>MASS::lda()</code></h3>
<pre class="r"><code>canal_lda&lt;-MASS::lda(stage~antseg1 + antseg2 + antseg3 + antseg4, data = canal_data)

canal_lda</code></pre>
<pre><code>## Call:
## lda(stage ~ antseg1 + antseg2 + antseg3 + antseg4, data = canal_data)
## 
## Prior probabilities of groups:
##         1         2         3         4         5         6 
## 0.1111111 0.1111111 0.1111111 0.2222222 0.2222222 0.2222222 
## 
## Group means:
##      antseg1    antseg2    antseg3    antseg4
## 1 -1.3073501 -1.3594034 -1.4443907 -1.4868663
## 2 -1.1332171 -1.1490818 -1.1510214 -1.2348777
## 3 -0.8167818 -0.8640406 -0.8315750 -0.9156920
## 4 -0.3028084 -0.2399942 -0.1731241 -0.0805297
## 5  0.4283631  0.5487119  0.6043044  0.6886356
## 6  1.5031198  1.3775452  1.2823132  1.2106121
## 
## Coefficients of linear discriminants:
##                LD1        LD2        LD3       LD4
## antseg1 14.5047455 -7.2205800  3.8168030  2.764369
## antseg2 -3.8449898  1.2049129 -8.6374675 -2.428811
## antseg3 -1.7878986  0.9023135  4.3236873 -3.984543
## antseg4 -0.2461576  5.5171016  0.5203163  3.569601
## 
## Proportion of trace:
##    LD1    LD2    LD3    LD4 
## 0.9719 0.0276 0.0003 0.0002</code></pre>
<pre class="r"><code>names(canal_lda)</code></pre>
<pre><code>##  [1] &quot;prior&quot;   &quot;counts&quot;  &quot;means&quot;   &quot;scaling&quot; &quot;lev&quot;     &quot;svd&quot;     &quot;N&quot;      
##  [8] &quot;call&quot;    &quot;terms&quot;   &quot;xlevels&quot;</code></pre>
</div>
<div id="explanation-of-the-output" class="section level3">
<h3>Explanation of the output</h3>
<div id="prior" class="section level4">
<h4>Prior</h4>
<p>From the raw data, the break down in proportions of the number of individuals in each group</p>
<pre class="r"><code>canal_lda$prior</code></pre>
<pre><code>##         1         2         3         4         5         6 
## 0.1111111 0.1111111 0.1111111 0.2222222 0.2222222 0.2222222</code></pre>
</div>
<div id="group-means" class="section level4">
<h4>Group means</h4>
<p>The mean value for each of the variables for each group</p>
<pre class="r"><code>canal_lda$means</code></pre>
<pre><code>##      antseg1    antseg2    antseg3    antseg4
## 1 -1.3073501 -1.3594034 -1.4443907 -1.4868663
## 2 -1.1332171 -1.1490818 -1.1510214 -1.2348777
## 3 -0.8167818 -0.8640406 -0.8315750 -0.9156920
## 4 -0.3028084 -0.2399942 -0.1731241 -0.0805297
## 5  0.4283631  0.5487119  0.6043044  0.6886356
## 6  1.5031198  1.3775452  1.2823132  1.2106121</code></pre>
</div>
<div id="coefficients" class="section level4">
<h4>Coefficients</h4>
<p>These are the coffeicients to describe the linear discrimination. For example: LD1 = 14.5047455 x Group1 + -3.8449898 x Group2 + -1.7878986 x Group3 + …</p>
<pre class="r"><code>canal_lda$scaling</code></pre>
<pre><code>##                LD1        LD2        LD3       LD4
## antseg1 14.5047455 -7.2205800  3.8168030  2.764369
## antseg2 -3.8449898  1.2049129 -8.6374675 -2.428811
## antseg3 -1.7878986  0.9023135  4.3236873 -3.984543
## antseg4 -0.2461576  5.5171016  0.5203163  3.569601</code></pre>
<p>You can also standardize the coefficients (loadings), which provides the relative strength of each factor on linear discriminant analysis</p>
<pre class="r"><code>loadings &lt;-canal_lda$scaling
col.ss &lt;- colSums(loadings^2)
sweep(loadings,2,sqrt(col.ss),&quot;/&quot;)</code></pre>
<pre><code>##                 LD1         LD2         LD3        LD4
## antseg1  0.95969829 -0.78391461  0.36703621  0.4257438
## antseg2 -0.25440157  0.13081342 -0.83060701 -0.3740641
## antseg3 -0.11829530  0.09796121  0.41577985 -0.6136643
## antseg4 -0.01628688  0.59897356  0.05003531  0.5497585</code></pre>
</div>
</div>
<div id="variation-expained" class="section level3">
<h3>Variation expained</h3>
<p>The amount of variation explained by the each of the linear discriminant axis</p>
<pre class="r"><code>prop = canal_lda$svd^2/sum(canal_lda$svd^2)
prop</code></pre>
<pre><code>## [1] 0.9719389713 0.0275790703 0.0003069345 0.0001750239</code></pre>
</div>
<div id="graphing-the-results" class="section level3">
<h3>Graphing the results</h3>
<pre class="r"><code># extracting the data
lda_pred &lt;- predict(canal_lda)
lda_scores&lt;-data.frame(stage = lda_pred$class, lda_pred$x)

#developing hulls for each stage group

hull1&lt;- lda_scores[lda_scores$stage ==1,][chull(lda_scores[lda_scores$stage ==1,c(&quot;LD1&quot;, &quot;LD2&quot;)]),]
hull2&lt;- lda_scores[lda_scores$stage ==2,][chull(lda_scores[lda_scores$stage ==2,c(&quot;LD1&quot;, &quot;LD2&quot;)]),]
hull3&lt;- lda_scores[lda_scores$stage ==3,][chull(lda_scores[lda_scores$stage ==3,c(&quot;LD1&quot;, &quot;LD2&quot;)]),]
hull4&lt;- lda_scores[lda_scores$stage ==4,][chull(lda_scores[lda_scores$stage ==4,c(&quot;LD1&quot;, &quot;LD2&quot;)]),]
hull5&lt;- lda_scores[lda_scores$stage ==5,][chull(lda_scores[lda_scores$stage ==5,c(&quot;LD1&quot;, &quot;LD2&quot;)]),]
hull6&lt;- lda_scores[lda_scores$stage ==6,][chull(lda_scores[lda_scores$stage ==6,c(&quot;LD1&quot;, &quot;LD2&quot;)]),]

all_hulls &lt;- rbind(hull1, hull2, hull3, hull4, hull5, hull6)

#plot using gglot2

ggplot(data = lda_scores) + 
  geom_point(aes(x = LD1, y = LD2, color = stage), size = 2) +
  geom_polygon(data = all_hulls,aes(x = LD1, y = LD2, color = stage, fill = stage), alpha = 0.25) +
  coord_equal() + 
  theme_classic()</code></pre>
<p><img src="/post/2017-11-17-Differencesbetweengroups_files/figure-html/unnamed-chunk-10-1.png" width="576" /></p>
</div>
<div id="assessing-the-accuracy-of-the-predicitions" class="section level3">
<h3>Assessing the accuracy of the predicitions</h3>
<pre class="r"><code>ct &lt;- table(canal_data$stage, lda_pred$class)
ct</code></pre>
<pre><code>##    
##      1  2  3  4  5  6
##   1  9  1  0  0  0  0
##   2  0 10  0  0  0  0
##   3  0  0 10  0  0  0
##   4  0  0  0 20  0  0
##   5  0  0  0  0 20  0
##   6  0  0  0  0  0 20</code></pre>
<pre class="r"><code># percent correct for each stage
diag(prop.table(ct, 1))</code></pre>
<pre><code>##   1   2   3   4   5   6 
## 0.9 1.0 1.0 1.0 1.0 1.0</code></pre>
<pre class="r"><code># total percent correct
sum(diag(prop.table(ct)))</code></pre>
<pre><code>## [1] 0.9888889</code></pre>
</div>
</div>
<div id="manova-and-mancova" class="section level2">
<h2>MANOVA and MANCOVA</h2>
<p>Two common multivariate regression models are the multivariate analysis of variance (MANOVA; an extension of the univariate ANOVA) when all predictor variables are categorical, and the multivariate analysis of covariance (MANCOVA; extension of univariate ANCOVA) when predictor variables are categorical and continuous.</p>
<div id="load-the-data-1" class="section level3">
<h3>Load the data</h3>
<pre class="r"><code>bryc_data &lt;- read.table(&quot;http://ecology.msu.montana.edu/labdsv/R/labs/lab1/bryceveg.R&quot;)


bryc_data %&gt;% rownames_to_column(&quot;site&quot;) -&gt; bryc_data

bryc_site_data &lt;- read.table(&quot;http://ecology.msu.montana.edu/labdsv/R/labs/lab2/brycesite.R&quot;)

bryc_site_data %&gt;% 
  rownames_to_column(&quot;site&quot;) %&gt;% 
  full_join(bryc_data, by = &quot;site&quot;) -&gt; bryc_site_data

glimpse(bryc_site_data)</code></pre>
<pre><code>## Observations: 160
## Variables: 182
## $ site     &lt;chr&gt; &quot;bcnp__1&quot;, &quot;bcnp__2&quot;, &quot;bcnp__3&quot;, &quot;bcnp__4&quot;, &quot;bcnp__5&quot;...
## $ plotcode &lt;int&gt; 50001, 50002, 50003, 50004, 50005, 50006, 50007, 5000...
## $ annrad   &lt;int&gt; 241, 222, 231, 254, 232, 216, 288, 279, 187, 279, 271...
## $ asp      &lt;int&gt; 30, 50, 50, 360, 300, 330, 150, 220, 360, 125, 125, 2...
## $ av       &lt;dbl&gt; 1.00, 0.96, 0.96, 0.93, 0.48, 0.76, 0.25, 0.00, 0.93,...
## $ depth    &lt;chr&gt; &quot;deep&quot;, &quot;shallow&quot;, &quot;shallow&quot;, &quot;shallow&quot;, &quot;shallow&quot;, &quot;...
## $ east     &lt;dbl&gt; 388220.9, 388477.2, 388384.0, 388307.5, 389026.3, 389...
## $ elev     &lt;int&gt; 8720, 8360, 8560, 8660, 8480, 8560, 8560, 8680, 8240,...
## $ grorad   &lt;int&gt; 162, 156, 159, 166, 159, 155, 169, 169, 146, 167, 168...
## $ north    &lt;int&gt; 4144784, 4147573, 4147347, 4146971, 4146846, 4146853,...
## $ pos      &lt;chr&gt; &quot;ridge&quot;, &quot;mid_slope&quot;, &quot;mid_slope&quot;, &quot;ridge&quot;, &quot;up_slope...
## $ quad     &lt;chr&gt; &quot;pc&quot;, &quot;pc&quot;, &quot;pc&quot;, &quot;pc&quot;, &quot;pc&quot;, &quot;pc&quot;, &quot;pc&quot;, &quot;pc&quot;, &quot;bp&quot;,...
## $ slope    &lt;int&gt; 9, 2, 2, 0, 2, 2, 3, 2, 3, 3, 2, 6, 3, 4, 3, 4, 2, 1,...
## $ junost   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ ameuta   &lt;dbl&gt; 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ arcpat   &lt;dbl&gt; 1.0, 0.5, 1.0, 1.0, 4.0, 1.0, 4.0, 2.0, 0.0, 5.0, 2.0...
## $ arttri   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ atrcan   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ berfre   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ ceamar   &lt;dbl&gt; 0.5, 0.0, 0.5, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5...
## $ cerled   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ cermon   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ chrdep   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ chrnau   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ chrpar   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ chrvis   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ eurlan   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ juncom   &lt;dbl&gt; 0.0, 2.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.5, 0.0, 0.0...
## $ pacmyr   &lt;dbl&gt; 0.0, 0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5...
## $ pruvir   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ purtri   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0...
## $ quegam   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ rhutri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ ribcer   &lt;dbl&gt; 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ roswoo   &lt;dbl&gt; 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ samcoe   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ shearg   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ sherot   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ symore   &lt;dbl&gt; 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5...
## $ arcuva   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ artarb   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ artfri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ artpyg   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ atrcon   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ berrep   &lt;dbl&gt; 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0...
## $ ericor   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ gutsar   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ tetcan   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ agrcri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ agrdas   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ agrscr   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ agrsmi   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ bougra   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ broano   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ brocil   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ broine   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ carrss   &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5...
## $ elysal   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ fesovi   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ hiljam   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ junbal   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ koenit   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ muhmon   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ muhric   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ oryhym   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ orymic   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ phlpra   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ poacom   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ poafen   &lt;dbl&gt; 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ poanev   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ poapra   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ sithys   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0...
## $ sticom   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ stilet   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ stipin   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ achmil   &lt;dbl&gt; 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ agogla   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ anemul   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ antros   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ antros.1 &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ apoand   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ arahol   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ arapen   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ arefen   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ artcar   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ artlud   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ astagr   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ astchi   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ astcon   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0...
## $ asthum   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ astken   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ astmeg   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ astmis   &lt;dbl&gt; 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0...
## $ astten   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ balsag   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ calnut   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ caschr   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ caslin   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0...
## $ chadou   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ cirneo   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ compal   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ corkin   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ creint   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ crycon   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ cympur   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ dessop   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ drasub   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0...
## $ echtri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ eriala   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ erican   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ erieat   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ erifla   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ eripan   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ eripum   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ erirac   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ erisub   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ eriumb   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ eupfen   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ euplur   &lt;dbl&gt; 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.5, 0.0, 0.5, 0.0...
## $ euprob   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ fraves   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ genaff   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ gerfre   &lt;dbl&gt; 0.5, 0.5, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0...
## $ gerric   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ gilcon   &lt;dbl&gt; 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ haparm   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ heddru   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ hymaca   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ hymfil   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ hymric   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5...
## $ ipoagg   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ irimis   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ ivesab   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ leppun   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ lesint   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ leueri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ ligpor   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ linkin   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ linlew   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ litinc   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ litmul   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5...
## $ lotuta   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ lupkin   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ lupser   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ lyggra   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ lygspi   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ macgri   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0...
## $ molpar   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ oenbra   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ oencae   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ oencor   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ oenfla   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ oenlav   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ opueri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ orofas   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ pedcan   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0...
## $ pedsim   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ pencae   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5...
## $ pencom   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ penlei   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ penuta   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ phllon   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ phycha   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ potcon   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ potcri   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ potgra   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ pteand   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0...
## $ pyrvir   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0...
## $ salibe   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ sclwhi   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ senmul   &lt;dbl&gt; 0.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5, 0.0, 0.5, 0.5...
## $ sphcoc   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ stapin   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ steten   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ strcor   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ swerad   &lt;dbl&gt; 0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0...
## $ taroff   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ thafen   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ towmin   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ tradub   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ valacu   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ vicame   &lt;dbl&gt; 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</code></pre>
</div>
<div id="using-manova-and-mancova" class="section level3">
<h3>Using MANOVA and MANCOVA</h3>
</div>
<div id="manova" class="section level3">
<h3>MANOVA</h3>
<p>-Independent variable is a categorical variable
- Interpret like ANOVA
- degfault test statistic is “Pillai” but “Wilks”, “Hotelling-Lawley”, “Roy” are also available. See <code>?summary.manova</code>. Wilks’ statistic is most popular in the literature, but the default Pillai–Bartlett statistic is recommended by <a href="https://www.crcpress.com/Multivariate-Analysis-of-Variance-and-Repeated-Measures-A-Practical-Approach/Hand-Taylor/p/book/9780412258008">Hand and Taylor (1987)</a></p>
<p>As a <strong>reminder</strong>:</p>
<p>-R provides Type I sequential SS, not the default Type III marginal SS reported by SAS and SPSS. In a nonorthogonal design with more than one term on the right hand side of the equation order will matter (i.e., A+B and B+A will produce different results)!
- Type I sequential SS: tests the main effect of factor A, followed by the main effect of factor B after the main effect of A, followed by the interaction effect AB after the main effects.
- Type II: tests for each main effect after the other main effect. No significant interaction is assumed (in other words, you should test for interaction first (SS(AB | A, B)) and only if AB is not significant, continue with the analysis for main effects).
- Type III: tests for the presence of a main effect after the other main effect and interaction. This approach is therefore valid in the presence of significant interactions. If the interactions are not significant, type II gives a more powerful test.</p>
<ul>
<li><p>NOTE: when data is balanced, the factors are orthogonal, and types I, II and III all give the same results.</p></li>
<li><p>If you wish to find ways to produce Type II or III SS then you will need to find a package to help. For example, <code>regr0</code> has a <code>drop1()</code> function for <code>mlm</code> models. Install via <code>install.packages(&quot;regr0&quot;, repos=&quot;http://R-Forge.R-project.org&quot;)</code></p></li>
</ul>
<pre class="r"><code>man_out &lt;- manova(cbind(junost,ameuta,arcpat,arttri,atrcan) ~ pos,data = bryc_site_data)
summary(man_out)</code></pre>
<pre><code>##            Df  Pillai approx F num Df den Df  Pr(&gt;F)  
## pos         4 0.18304    1.477     20    616 0.08253 .
## Residuals 155                                         
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="mancova" class="section level3">
<h3>MANCOVA</h3>
<ul>
<li>When you have a categorical and continuous independent variables</li>
</ul>
<pre class="r"><code>man_out &lt;- manova(cbind(arcpat, artarb, carrss, purtri,sticom) ~ pos + elev, data = bryc_site_data)
summary(man_out)</code></pre>
<pre><code>##            Df Pillai approx F num Df den Df    Pr(&gt;F)    
## pos         4 0.2700   2.2150     20    612 0.0018133 ** 
## elev        1 0.1438   5.0387      5    150 0.0002688 ***
## Residuals 154                                            
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
<div id="anosim" class="section level2">
<h2>ANOSIM</h2>
<p>Analysis of similarities (ANOSIM) provides a way to test statistically whether there is a significant difference between two or more groups of sampling units.</p>
<p>The R-statistic in ANOSIM is a ratio between within-group and between-group dissimilarities</p>
<pre class="r"><code># create the y variable 
y&lt;-cbind(bryc_site_data$arcpat,bryc_site_data$artarb,bryc_site_data$carrss,bryc_site_data$purtri,bryc_site_data$sticom)

# convert to an x variable
x &lt;- as.factor(bryc_site_data$pos)

# analysis
anosim_out &lt;- anosim(y, x, distance = &quot;euclidean&quot;, permutations = 999)
summary(anosim_out)</code></pre>
<pre><code>## 
## Call:
## anosim(x = y, grouping = x, permutations = 999, distance = &quot;euclidean&quot;) 
## Dissimilarity: euclidean 
## 
## ANOSIM statistic R: 0.0162 
##       Significance: 0.202 
## 
## Permutation: free
## Number of permutations: 999
## 
## Upper quantiles of permutations (null model):
##    90%    95%  97.5%    99% 
## 0.0260 0.0344 0.0444 0.0548 
## 
## Dissimilarity ranks between and within classes:
##            0%     25%     50%       75%    100%    N
## Between   198 2978.50 6359.50  9536.000 12720.0 9823
## bottom    198 4155.00 6932.50  9591.125 12485.0  190
## low_slope 198 4070.00 6876.75 10634.500 12716.5  528
## mid_slope 198 3220.75 7323.50 10180.000 12704.0 1431
## ridge     198 2978.50 6196.50  8470.000 11827.5  153
## up_slope  198 1628.50 3863.50  6932.500 11978.0  595</code></pre>
<pre class="r"><code>perms&lt;-permustats(anosim_out)
summary(perms)</code></pre>
<pre><code>## 
##   statistic     SES    mean lower  median   upper Pr(perm)
## R    0.0162  0.7900  0.0002       -0.0011  0.0344    0.202
## 
## (Interval (Upper - Lower) = 0.95)</code></pre>
<pre class="r"><code>densityplot(perms)</code></pre>
<p><img src="/post/2017-11-17-Differencesbetweengroups_files/figure-html/unnamed-chunk-15-1.png" width="576" /></p>
<pre class="r"><code>anosim_out$perm</code></pre>
<pre><code>##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03441343256 -0.00694466724 -0.00460821364 -0.03034884174  0.00835885965 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00200030003 -0.01695583102  0.00903738667  0.00676980835 -0.02681406353 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02178813532 -0.00715582623  0.04245261951 -0.00582231630  0.06835327724 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00374871329 -0.01638827755 -0.00364743850 -0.00762495128  0.01328049099 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.04088904504 -0.02256403654  0.01888353087 -0.01591317862 -0.02274265546 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02023049959 -0.00629615018  0.00560542942  0.01180926563  0.00889896842 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.07256753125  0.01638507977  0.01211576067  0.00089927934  0.00081880770 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00685035027 -0.01365877797  0.01787928699 -0.00553012343 -0.01247956978 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03059538716  0.01145849363 -0.02533746168 -0.02926110415 -0.02625290563 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00911019769 -0.01600630082 -0.00662738409  0.00262140754 -0.03233582354 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.05317495578  0.00674159056 -0.00458994060 -0.00639492999 -0.02127642004 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00028825714  0.00352497402  0.00212944120  0.00529429585  0.03594576718 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03382430286 -0.00097732629 -0.02154078167  0.01241835511  0.02493345189 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03116438138  0.02076825395 -0.01566406795  0.00672363379 -0.00814900086 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.04441802507 -0.01360810544 -0.00460241546  0.01905603535 -0.00037969260 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02662188039  0.01957049159 -0.03002031364 -0.00028600815 -0.00793601458 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01438369039 -0.01288234966 -0.01576931361  0.00814390550  0.01488247398 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00618447382  0.00430909810  0.00540544510  0.01796162107  0.00046610297 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01831232280  0.00859844726 -0.00368820143 -0.02498212142 -0.01045077084 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01743549820 -0.03007555443 -0.00730830066  0.02822864248  0.01506812100 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00879649886  0.02077637139 -0.00464229988 -0.02279413622 -0.01192733755 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03995132204  0.01086546333  0.00922517725  0.02135942179 -0.00536313600 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00477952335 -0.02409545750 -0.00871103728 -0.01212964114  0.00953567830 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01269933818  0.00469546738 -0.00580102119 -0.00406585588  0.03118954195 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.05364819226 -0.01370228186  0.00196343769 -0.02382708985 -0.01376086099 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03829090047  0.00688879392  0.04867750485  0.02285088806 -0.00565972143 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02151245847  0.01586032738 -0.01159754440 -0.01883082019 -0.00581159847 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02389125632  0.00860459684  0.00740873207  0.01501941633 -0.00111173852 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00195226303 -0.01157501937  0.05976315826 -0.01695632298 -0.03720766086 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01870716093 -0.01057189999 -0.01101126107 -0.02377276271  0.01912083435 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01258478030  0.01016384904 -0.00472119020 -0.02052817437  0.03761349795 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00651373986  0.02688416874  0.01368200581  0.02771158585  0.00318685258 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00700233273 -0.01332237841 -0.01493887441 -0.02026795931  0.01978607827 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02373997667 -0.00478075326  0.02350794426 -0.03852479533 -0.00933931344 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00029159548  0.02353925440  0.01050952568  0.03499764260  0.01894453469 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00604008169  0.03434613860  0.02208566954 -0.00970220890  0.03995146260 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03299379339  0.02651220704  0.02523418389 -0.00018213297 -0.00394985724 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00903418889  0.02282976864 -0.00041697662  0.02753402114 -0.01966206761 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01429313344  0.01260846496 -0.01766521135 -0.00568168421 -0.00710610249 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00746355118  0.00005819259 -0.00705275928 -0.01129614473  0.00631343928 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00861454159 -0.01585589968  0.00862251847 -0.02483600741  0.00381695605 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.06565888297  0.01233257726 -0.04391657080  0.02379447951 -0.01456684243 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00671822216  0.06458618549  0.02960414525  0.01231325001  0.00653911127 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00603358071  0.00251637273  0.00837453229 -0.01969415085  0.01745032748 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02399323392 -0.02433529109  0.01445369017 -0.00857550055 -0.01113098460 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01934344912 -0.01287837879 -0.01790233913  0.00918863118 -0.01765628567 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00823709798 -0.00656806701  0.01105747077  0.00306122546 -0.00990236893 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01197983739 -0.00899722113 -0.01737136688  0.00725077573 -0.04201276646 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02256263092 -0.00840215269  0.01148488411  0.01576414796  0.01328516467 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.06663873938 -0.02263530138  0.02585349924 -0.00746042368 -0.01190003342 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03801694550  0.00305866020  0.00408275844 -0.01853448074 -0.02672385799 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00213320122  0.01031509355  0.01554258740 -0.00917011216 -0.01680841681 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00349584259  0.01453226422 -0.01150150554  0.00937589465  0.01985375879 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02754677713  0.02221231574  0.04373721393 -0.01781828316 -0.01426783231 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00177332784  0.01163328224 -0.01838608261  0.01157986875 -0.00542484263 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02057199451 -0.01242288823  0.01597014130  0.01970785562 -0.00406880768 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.04693945802 -0.00013072249 -0.01848907928 -0.00272001166  0.03579311705 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01173002391 -0.01259830937  0.00708579131 -0.03204486058 -0.02311229789 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02414208888 -0.00265812932  0.02020424967 -0.01898455967  0.01687820575 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02327267892 -0.03199334468 -0.00611949912  0.05376064172  0.01549409358 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00412534867  0.02073796287  0.03425677642 -0.00049713199 -0.01640915098 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01580564883 -0.03313656905  0.02933338806  0.00504160085 -0.00176043129 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00907098094  0.01104717462 -0.01083120139  0.00751366147 -0.00572613688 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01557895777 -0.01028055049 -0.02880216982 -0.01434956901  0.00071075081 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.04309438961 -0.01803984372 -0.03511364124 -0.02022941023  0.03847542300 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00027511461 -0.01507427058  0.01931509078  0.01278016122 -0.01578045313 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02668256796 -0.00527658506 -0.00935470496  0.00832589791 -0.00244338601 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00052127349 -0.01531382305 -0.01299761034  0.03366603026 -0.03528164775 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00366258404  0.02711444413 -0.01507044027 -0.01409669830  0.04896404011 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00471082376 -0.00569335084  0.01009922575 -0.02233783744 -0.01625903096 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00921340520  0.03455490803  0.00686419561  0.00983243942  0.00127296292 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01067942977 -0.00910162342 -0.01250704961  0.01774227436 -0.00480514074 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01684433036 -0.00824373953 -0.02604143038  0.01164192679  0.00552544975 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00934363572 -0.01803987886  0.01522621790  0.00882341645  0.00221824112 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02555361061  0.00135129100 -0.01764310800  0.00829163596  0.01720241158 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02935194222 -0.00668947727  0.00024629944  0.00901264779  0.00241801460 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00703279950  0.00745195483 -0.00872435551 -0.00710662959  0.01051005279 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00102606610  0.02999251754 -0.01015207699 -0.01456097398 -0.02892797265 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01478253453  0.02626225299 -0.01181070639  0.01308012013 -0.02220047341 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00490645067  0.00392764848  0.03730914649 -0.00537958173  0.01865399343 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00584150299 -0.00865642901  0.04454129778  0.01511914494 -0.01839304042 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03118701184 -0.01146365927 -0.02746258060 -0.01643076236 -0.02229292794 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00386320089 -0.00355080225 -0.00056168501  0.02850604122 -0.00789785204 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00222744792  0.02242842953 -0.01426930821  0.03842000650  0.00502220332 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01859833095 -0.01900571422 -0.04193791729  0.00439895224 -0.00915320960 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01496979801 -0.03970836094 -0.00872702618  0.03398573811 -0.00603607568 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01631269044  0.03679191415 -0.00035657018  0.01144134508  0.00887805985 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00241471140  0.00430442442 -0.00897701537 -0.01790961320  0.02815456641 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02156393923  0.01833597232  0.02044461037  0.01787552696  0.01809167589 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00434058395  0.01185357774  0.00427090043 -0.00173333800 -0.02877588477 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00076950565  0.01327613358 -0.02553807853  0.01559723081  0.01447435276 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01054979664 -0.00456485032  0.00862251847 -0.01037711645  0.00724283399 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00282388684 -0.00971805725  0.01302990442 -0.01996090203 -0.01569291826 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02408249067  0.00812334833  0.00140944845  0.02057332985 -0.01458645080 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01276238015  0.01501414526 -0.00265777791  0.00453090464  0.00810222892 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.07632365918  0.02932126460 -0.00703294006  0.00513707746 -0.01345661495 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00201428593 -0.02902295729  0.00793179772  0.00331606403  0.02083972963 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03015567467 -0.01049167433  0.01348521928 -0.01331643968 -0.01991015921 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02375814428 -0.01897949945 -0.02222651248  0.01292040677 -0.00225366270 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01549693995  0.01919758110 -0.02639107790  0.01285451842 -0.00852609307 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01653273996 -0.02786167073 -0.02490843189  0.00475622523 -0.01114374058 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01618699304 -0.00035973282  0.00402695540  0.00790828876 -0.00662049656 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00023319205  0.04674309317 -0.01773173223  0.02280492434  0.01821227793 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01030722209 -0.01494681615 -0.00730587597 -0.01196395391 -0.00794588904 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00886200066  0.04779365217  0.02876857555 -0.01701869729 -0.00005401088 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02776359372 -0.01582118092  0.00539289996 -0.01375298953 -0.00005791147 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00545745298 -0.01821937630  0.00007154596 -0.02918425197 -0.00367154485 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00373989303  0.02457853331  0.03013170888 -0.02220251155 -0.01594431306 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03518220026 -0.02143876894  0.00948142144  0.02352579561 -0.02943518995 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.05465025743 -0.01757739536 -0.02088850458  0.00642125019  0.00790143637 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01571962500 -0.00666983376  0.00934862566 -0.00759968530  0.01424284745 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01693776882  0.00164727903  0.00471296733 -0.01853630805 -0.01941611958 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02995389818  0.00482257040  0.01154588793 -0.03259480868  0.02607867926 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01226900818 -0.00114227558 -0.01462542157  0.02891795762  0.08038782832 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02858465042  0.01331830212  0.02821353209  0.01595260621  0.00301368042 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00140164726  0.01342115823  0.01330927102 -0.01794918135  0.03272855325 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00639738982 -0.00745560944  0.00699562090 -0.01661988828 -0.00817468854 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00777204922  0.03078992471 -0.02194043405 -0.00229850192 -0.00133189346 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01083070942 -0.02375684409 -0.02249547751  0.00419338059  0.01679534456 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02008016873  0.01518865275 -0.01936456853 -0.00062029928 -0.00027578228 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01696900869 -0.02280295648 -0.02287467814 -0.01940262565  0.01029594201 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01301437234 -0.01674122827  0.00988971836 -0.00778677307  0.00511708254 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02018959610 -0.01154715299 -0.01546422419 -0.00576226127  0.03598407027 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00627151672  0.00850409515  0.00477860970 -0.02182816030 -0.01028610268 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00655671664  0.02100734959 -0.02447156577 -0.00274288809 -0.00918093542 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00984287614 -0.01470125467 -0.00148412191  0.00274011199 -0.00483307740 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00135540243 -0.01292479932 -0.03073433251 -0.00885072058 -0.02134733348 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01842269896 -0.02481710185 -0.01715760750  0.01608982968 -0.01839135368 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00845486337 -0.00336076268  0.00537740302  0.00469490514  0.00858758886 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03696171282 -0.00344386986  0.00886537415 -0.00843040561 -0.00872456635 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01958901061 -0.00424468565 -0.00501387503 -0.00471089404 -0.02274328799 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02358405848  0.02289572728 -0.01418616590 -0.02822309029 -0.02156288502 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00483813762  0.00438830468  0.00887089120  0.00002403607 -0.00471570829 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01622726399  0.02323353245 -0.01630025072 -0.00770489581 -0.01992822141 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02714399725  0.01589357025  0.04384435717  0.08066740576  0.03203923811 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01534007297 -0.01773988481 -0.00192014465 -0.02463985340  0.01656183625 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01981679103 -0.02831705587  0.00637735976  0.00354496894 -0.00204085211 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02262834357 -0.00592706999  0.00329975886  0.02724414754 -0.00852669046 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00035815150 -0.00459022173 -0.01549985661 -0.00536317114  0.01734195432 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00834884462  0.01885239643 -0.00193486850  0.00189108350  0.01297944273 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.04063578779 -0.00858611296 -0.00230275391  0.02388981556  0.00235500777 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01785472381 -0.01484884457 -0.00567750249  0.02291101337 -0.00085454555 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01388184957  0.01406299861 -0.00908654816  0.01445116006  0.00231853198 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00149006065 -0.01623854408  0.02477433591  0.00007425178  0.00042136918 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00076448056 -0.00818825275 -0.01445892610  0.00031471790 -0.02122451759 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01342941624  0.02975447611  0.02539164826 -0.03052946367  0.02349216619 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00459763636 -0.00961066802  0.03273751406  0.00593954486 -0.00743111654 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01397778301  0.01761172758  0.00178362399  0.02126152049  0.00368943134 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01818384930  0.00166874985 -0.00552801501 -0.00440562892 -0.01121493514 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01763383092  0.01488525008  0.03003514291 -0.01113168741 -0.00481923206 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03178566460 -0.03640192540 -0.00769076935  0.00879143863 -0.00581982133 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02650465184 -0.00154196310 -0.01393666868  0.01703180468 -0.01503291026 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00544213174 -0.00189972805 -0.00276934885 -0.02050174875  0.02544239107 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01094161270  0.00680923594  0.00515067682  0.02110328303  0.00469339410 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00039508412 -0.03766501386 -0.00831672625 -0.02296467285 -0.05060931613 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01708788884  0.00308561293  0.03343867153  0.00664527058  0.04569752412 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01241108104  0.01840073618 -0.02726308825  0.00078591624 -0.02040922393 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01094038278 -0.00080685995 -0.00068783923  0.02927378985  0.00562285909 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01985934612 -0.01362817064 -0.01543822025  0.00801887576  0.00400566028 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01917069865 -0.01234477100 -0.01856491238 -0.03195286288  0.03135347216 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01903530249 -0.02947215771  0.01119887596  0.02142485332 -0.01005498392 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01987670550 -0.01704884780  0.03011340070  0.00957921732  0.02172231726 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00177645534 -0.01342484798  0.00274046340 -0.03747746926  0.00786316842 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.05168134595  0.00309225448 -0.01228794889  0.02109007022  0.00177371439 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00009329790  0.01331064150 -0.00655917647  0.06470984475 -0.03247237934 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01034187058  0.01202133827  0.03692959445 -0.01323881442  0.00839456235 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01799099849  0.03892128507 -0.01598180793  0.02114770056 -0.02052487117 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00446234562  0.03432505432  0.02357014286 -0.01506650454  0.01241006196 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00318221404 -0.00081518824  0.00385975712 -0.00021063188  0.04390427164 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03650875238  0.02733097960  0.00226575101  0.02093647130 -0.00472379059 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.03183145261  0.01064857645  0.02981804519 -0.00424349087 -0.00801163683 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02772434184  0.01893522247 -0.00730668420 -0.01334803095  0.01167436143 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02244413731  0.00412123723 -0.02582598426 -0.03350217033 -0.02811974222 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01816364354  0.05345365471 -0.01479905055 -0.01988703680 -0.04081092781 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00252336568 -0.00866978238 -0.00403243731  0.00600033784 -0.03304214665 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01169024492  0.00580875209 -0.02667852680  0.04783799942  0.02602723364 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01167903511 -0.02575981479 -0.01038516362 -0.00779109535 -0.00865284468 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00738796406  0.01214292424 -0.02636198160 -0.02566356509 -0.00339987401 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00241920937 -0.02132976325 -0.02581951842 -0.00160451310 -0.01596479995 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00123898211 -0.02957114837  0.00999696703 -0.02525242178 -0.03407759525 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01950864439  0.01481890490 -0.03552780662  0.04273061564 -0.03322691516 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.05586703077 -0.02153097749  0.02023555981 -0.01965915096 -0.01274625068 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02441537618  0.05378221795  0.02253631072 -0.02996830577  0.02366927408 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01756119561 -0.03089488925  0.01514961171 -0.02850294886  0.01288965887 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02346085605 -0.00179001956  0.02456581246 -0.01349519916  0.00251240186 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.02464350801 -0.02744768105  0.00429068450 -0.01057049437  0.01742439382 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00034212745  0.00751756206  0.02224084979 -0.01138972376 -0.02129609870 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01520931534 -0.00858280976  0.00156568290 -0.00802622012 -0.00538140903 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.01620519579 -0.01717514259 -0.01010597271  0.00693482792  0.00395407410 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.03640312018 -0.02269700801 -0.03741375962  0.01632418137  0.00180245928 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00218482255 -0.01545698526 -0.01263942370 -0.01430877094  0.00324880520 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00132423285  0.01484462772 -0.03409674680 -0.03769013928 -0.03375440850 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00072979694  0.01106330409 -0.02465967262 -0.03337085045 -0.01672509880 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01446307267  0.03100512485  0.01749513155 -0.01558883224 -0.01696036413 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.01857387319  0.02743316804 -0.00362280504 -0.01953682704  0.00803359961 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00163062246  0.01490528014  0.00540579651 -0.04297364701 -0.02963612306 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.02773179161 -0.00703501335  0.00485201810 -0.00348055649  0.02014338640 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
##  0.00054601237 -0.02503079094 -0.03393380052  0.01187554053  0.01353768397 
##           TRUE           TRUE           TRUE           TRUE           TRUE 
## -0.00477270610  0.00945278197  0.00903967080  0.02000338684 -0.00900287874 
##           TRUE           TRUE           TRUE           TRUE 
## -0.01154079257 -0.03376575887 -0.01526329108  0.01476380467</code></pre>
<p>R value is supposed to vary between 0 and 1 (not between -1 and +1) but you can obtained negative values but they are always close to 0. R value close to 1 indicates high separation between levels of your factor (e.g. control vs treatment samples), while R value close to 0 indicate no separation between levels of your factor.</p>
</div>
</div>

    
    

    

        <h4 class="page-header">Related</h4>

         <div class="item">

    
    
    

    
    

    <h4><a href="/post/2017-11-09-latentvariables_3/">Latent variable analysis. Part 3</a></h4>
    <h5>November 9, 2017</h5>
    
    <a href="/tags/applied-multivariate"><kbd class="item-tag">Applied Multivariate</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4><a href="/post/2017-10-20-latentvariables_2/">Latent variable analysis. Part 2</a></h4>
    <h5>October 20, 2017</h5>
    
    <a href="/tags/applied-multivariate"><kbd class="item-tag">Applied Multivariate</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4><a href="/post/2017-10-13-latentvariables_1/">Latent variable analysis. Part 1</a></h4>
    <h5>October 13, 2017</h5>
    
    <a href="/tags/applied-multivariate"><kbd class="item-tag">Applied Multivariate</kbd></a>
    

</div>
 

    

    

        <h4 class="page-header">Comments</h4>

        <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "snrrgroup" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    

</main>

        <footer>

            <p class="copyright text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a></p>

        </footer>
       
    </body>

</html>

