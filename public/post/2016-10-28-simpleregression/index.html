<!DOCTYPE html>
<html lang="en-us">
    <head>
         
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Simple Regression</title>
        <style>

    html body {
        font-family: 'Roboto', sans-serif;
        background-color: #FEFDFA;
    }

    :root {
        --accent: #D00000;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/hybrid.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/R.min.js"></script> 

    <script>hljs.initHighlightingOnLoad();</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.30.2" />
        
        
        
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-48496439-3"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-48496439-3');
        </script>
        
    </head>

    
    
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <body>
         
        <nav class="navbar navbar-default navbar-fixed-top">

            <div class="container">

                <div class="navbar-header">

                    <a class="navbar-brand visible-xs" href="#">Simple Regression</a>

                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>

                </div>

                <div class="collapse navbar-collapse">

                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/post/">Posts</a></li>
                            
                                <li><a href="/resources/">Resources</a></li>
                            
                        </ul>
                    

                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:cchizinski2@unl.edu"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/chrischizinski/"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://twitter.com/chrischizinski/"><i class="fa fa-twitter"></i></a></li>
                            
                        </ul>
                    

                </div>

            </div>

        </nav>


<main>

    <div class="item">

    
    
    

    
    

    <h4><a href="/post/2016-10-28-simpleregression/">Simple Regression</a></h4>
    <h5>October 28, 2016</h5>
    
    <a href="/tags/regression"><kbd class="item-tag">Regression</kbd></a>
    

</div>


    <br> <div class="text-justify"><p>The RMarkdown file for this lesson can be found <a href="https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/Rmd/2016-10-28-SimpleRegression.Rmd">here</a>.</p>
<p>This lesson will follow Chapter 5 in Quinn and Keough (2002).</p>
<p>Load the packages we will be using in this lesson</p>
<pre class="r"><code>library(MASS)
library(car)
library(RCurl)
library(mgcv)
library(tidyverse)
library(broom)
library(Rfit)
library(mgcv)
library(gtable)
library(lmodel2)
library(grid)</code></pre>
<div id="linear-regression-analysis" class="section level2">
<h2>Linear regression analysis</h2>
<p>Statistical models that assume a linear relationship between a single, continuous (usually) predictor value are <em>simple linear regression</em> models.</p>
<p>These models have three primary purposes:</p>
<ul>
<li>describe a linear relationship between \( Y \) and \( X \)</li>
<li>determine the amount of variation (explained) in \( Y \) with \( X \) and the amount of variation unexplained</li>
<li>predict values of \( Y \) from \( X \)</li>
</ul>
<div id="simple-bivariate-linear-regression" class="section level3">
<h3>Simple bivariate linear regression</h3>
<div id="linear-model-for-regression" class="section level4">
<h4>Linear model for regression</h4>
<p>Consider you have a set of observations (\( i = 1 :n \) ), where the each observation was chosen based on its \( X \) value and its \( Y \) value for each observation is sampled from a population of possible \( Y \) values.</p>
<p>This model can be represented as:</p>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i  \]</span></p>
<ul>
<li>\( y_i \) is the value of \( Y \) for the <em>i</em>th observation when the predictor \( X = x_i \)</li>
<li>\( _0 \) is the population intercept (i.e., mean value of the probability distribution) when \( x_i = 0\)</li>
<li>\( _1 \) is the population slope and measures the change in \( Y \) with a change in \( X \)</li>
<li>\( _i \) is the random or unexplained error associated with the <em>i</em>th observation</li>
</ul>
<p>In this model, the response variable \( Y \) is a random variable and \( X \) represents fixed values choed by the researcher. Thus repeated sampling, you would have the same values of \( X \) while \( Y \) would vary.</p>
</div>
<div id="estimating-model-parameters" class="section level4">
<h4>Estimating model parameters</h4>
<p>The main goal in regression analysis is estimating \( _0 \), \( <em>1 \), and \( </em>^2 \).</p>
<p>We discussed solving for \( _0 \) and \( _1 \) using OLS in an <a href="https://chrischizinski.github.io/SNR_R_Group/2016-10-07-REstimation2">earlier lesson</a></p>
<div id="regression-slope" class="section level5">
<h5>Regression slope</h5>
<p>The most informative of the parameters in a regression equation is \( _{1} \), because this describes the relationship between \( Y \) and \( X \).</p>
</div>
<div id="intercept" class="section level5">
<h5>Intercept</h5>
<p>The OLS regression line must pass through \( {x} \) and \( {y} \). We can then estimate \( <em>{0} \) by substituting in \( </em>{1} \), \( {x} \) and \( {y} \).</p>
<p>Often the intercept does not contain a lot of usable information because rarely do we have situations where \( X = 0 \).</p>
<p>Lets begin to explore this with the coarse woody debris data in lakes. <code>christ</code> data in <code>Chap 5</code> on github.</p>
<pre class="r"><code>cwd_data &lt;- read_csv(getURL(&#39;https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/ExperimentalDesignData/chpt05/christ.csv&#39;))

glimpse(cwd_data)</code></pre>
<pre><code>## Observations: 16
## Variables: 17
## $ LAKE     &lt;chr&gt; &quot;Bay&quot;, &quot;Bergner&quot;, &quot;Crampton&quot;, &quot;Long&quot;, &quot;Roach&quot;, &quot;Tende...
## $ AREA     &lt;int&gt; 69, 9, 24, 8, 20, 175, 254, 22, 240, 85, 12, 25, 58, ...
## $ CABIN    &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 1.9, 3.6, 4.1, 4.8, 6.0...
## $ RIP.DENS &lt;int&gt; 1270, 1210, 1800, 1875, 1300, 2150, 1330, 964, 961, 1...
## $ RIP.BASA &lt;int&gt; 53, 37, 37, 27, 43, 75, 86, 35, 33, 28, 47, 30, 31, 3...
## $ CWD.DENS &lt;int&gt; 442, 338, 965, 833, 613, 637, 298, 203, 48, 278, 316,...
## $ CWD.BASA &lt;int&gt; 121, 41, 183, 130, 127, 134, 65, 52, 12, 46, 54, 97, ...
## $ L10CABIN &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000...
## $ LCWD.BAS &lt;dbl&gt; 2.082785, 1.612784, 2.262451, 2.113943, 2.103804, 2.1...
## $ RESID1   &lt;dbl&gt; 51.393669, -21.675367, 52.170151, -9.493554, 53.92818...
## $ PREDICT1 &lt;dbl&gt; 69.60633, 62.67537, 130.82985, 139.49355, 73.07181, 1...
## $ RESID2   &lt;dbl&gt; 20.600685, -59.399315, 82.600685, 29.600685, 26.60068...
## $ PREDICT2 &lt;dbl&gt; 100.399315, 100.399315, 100.399315, 100.399315, 100.3...
## $ RESID3   &lt;dbl&gt; -0.968746, -80.968746, 61.031254, 8.031254, 5.031254,...
## $ PREDICT3 &lt;dbl&gt; 121.968746, 121.968746, 121.968746, 121.968746, 121.9...
## $ RESID4   &lt;dbl&gt; -0.11210186, -0.58210286, 0.06756414, -0.08094386, -0...
## $ PREDICT4 &lt;dbl&gt; 2.1948869, 2.1948869, 2.1948869, 2.1948869, 2.1948869...</code></pre>
<pre class="r"><code>ggplot(data = cwd_data) + 
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 2) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>`lm()’ is the function in R to conduct simple linear regression.</p>
<pre class="r"><code>mod_cwd &lt;- lm(CWD.DENS ~  RIP.DENS, data = cwd_data)

mod_cwd # displays the coefficients</code></pre>
<pre><code>## 
## Call:
## lm(formula = CWD.DENS ~ RIP.DENS, data = cwd_data)
## 
## Coefficients:
## (Intercept)     RIP.DENS  
##   -482.0245       0.6524</code></pre>
<pre class="r"><code>summary(mod_cwd) # displays a bunch more information</code></pre>
<pre><code>## 
## Call:
## lm(formula = CWD.DENS ~ RIP.DENS, data = cwd_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -283.65  -89.99  -20.71   92.69  272.69 
## 
## Coefficients:
##              Estimate Std. Error t value   Pr(&gt;|t|)    
## (Intercept) -482.0245   126.5724  -3.808    0.00192 ** 
## RIP.DENS       0.6524     0.0969   6.733 0.00000958 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 150.2 on 14 degrees of freedom
## Multiple R-squared:  0.764,  Adjusted R-squared:  0.7472 
## F-statistic: 45.33 on 1 and 14 DF,  p-value: 0.000009581</code></pre>
<p>There is a lot of information stored in our object <code>mod_cwd</code>.</p>
<pre class="r"><code>names(mod_cwd)</code></pre>
<pre><code>##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</code></pre>
<p>We can call on these directly from our <code>mod_cwd</code> or use several ‘helper’ functions.</p>
<pre class="r"><code># Coefficients
mod_cwd$coefficients</code></pre>
<pre><code>##  (Intercept)     RIP.DENS 
## -482.0245271    0.6524071</code></pre>
<pre class="r"><code>#or
coef(mod_cwd)</code></pre>
<pre><code>##  (Intercept)     RIP.DENS 
## -482.0245271    0.6524071</code></pre>
<pre class="r"><code># Residuals
mod_cwd$residuals[1:10]  # display the first 10 residuals</code></pre>
<pre><code>##          1          2          3          4          5          6 
##   95.46757   30.61199  272.69183   91.76130  246.89535 -283.65064 
##          7          8          9         10 
##  -87.67686   56.10412  -96.93865 -153.34535</code></pre>
<pre class="r"><code>#or
resid(mod_cwd)[1:10]</code></pre>
<pre><code>##          1          2          3          4          5          6 
##   95.46757   30.61199  272.69183   91.76130  246.89535 -283.65064 
##          7          8          9         10 
##  -87.67686   56.10412  -96.93865 -153.34535</code></pre>
<pre class="r"><code>mod_cwd$model</code></pre>
<pre><code>##    CWD.DENS RIP.DENS
## 1       442     1270
## 2       338     1210
## 3       965     1800
## 4       833     1875
## 5       613     1300
## 6       637     2150
## 7       298     1330
## 8       203      964
## 9        48      961
## 10      278     1400
## 11      316     1280
## 12      269      976
## 13        5      771
## 14       36      833
## 15       11      883
## 16       17      956</code></pre>
<p>The <code>broom</code> package makes inspection of the models a bit easier (although they are not too difficult) in base R. The biggest plus for broom, is that the outputs of the models are returned in a tidy format.</p>
<pre class="r"><code># tidy will give you a data.frame representation
tidy(mod_cwd)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept) -482.     127.         -3.81 0.00192   
## 2 RIP.DENS       0.652    0.0969      6.73 0.00000958</code></pre>
<pre class="r"><code># augment will give fitted values and residuals for each of the original points in the regression

head(augment(mod_cwd))</code></pre>
<pre><code>## # A tibble: 6 x 9
##   CWD.DENS RIP.DENS .fitted .se.fit .resid   .hat .sigma .cooksd .std.resid
##      &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1      442     1270    347.    37.6   95.5 0.0627   153. 0.0144       0.657
## 2      338     1210    307.    37.7   30.6 0.0631   156. 0.00149      0.211
## 3      965     1800    692.    65.4  273.  0.190    131. 0.476        2.02 
## 4      833     1875    741.    71.5   91.8 0.226    153. 0.0706       0.695
## 5      613     1300    366.    37.9  247.  0.0637   139. 0.0981       1.70 
## 6      637     2150    921.    95.2 -284.  0.402    118. 2.00        -2.44</code></pre>
<pre class="r"><code>#glance will let you see the statistics associated with the model
glance(mod_cwd)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.764         0.747  150.      45.3 9.58e-6     2  -102.  210.  212.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
</div>
<div id="confidence-intervals" class="section level5">
<h5>Confidence intervals</h5>
<p>Confidence intervals for \( _{1} \) are calculated in the usual manner when we know the standard error of a statistic and use the t distribution.</p>
<p>This can be represented as a confidence band (e.g. 95%) for the regression line. The 95% confidence band is a band that will contain the true population regression line 95% of the time.</p>
<p>We can display our confidence intervals using <code>geom_smooth</code> in ggplot.</p>
<pre class="r"><code>ggplot(data = cwd_data) + 
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 2) +
  geom_smooth(aes(x = RIP.DENS, y = CWD.DENS), method = &#39;lm&#39;, alpha = 0.35) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;, title = &quot;Linear model fit&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We can also use <code>geom_smooth</code> to explore other non-linear relationships between \( X \) and \( Y \).</p>
<pre class="r"><code>ggplot(data = cwd_data) + 
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 2) +
  geom_smooth(aes(x = RIP.DENS, y = CWD.DENS), method = &#39;loess&#39;, alpha = 0.35) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;, title = &quot;Loess fit&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># or via a GAM

ggplot(data = cwd_data) + 
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 2) +
  geom_smooth(aes(x = RIP.DENS, y = CWD.DENS), method = &#39;gam&#39;, alpha = 0.35) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;, title = &quot;GAM fit&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>While using <code>geom_smooth</code> makes nice visuals, I think you have a lot more flexibility when you build your own predictions. The <code>predict()</code> function is one of my favorite functions in R.</p>
</div>
<div id="predicted-values" class="section level5">
<h5>Predicted values</h5>
<p>Prediction from the OLS regression equation is straightforward by substituting an X-value into the regression equation and calculating the predicted Y-value. Do not predict from X-values outside the range of your data.</p>
<p>If we run the <code>predict()</code> with just the model, we get results the same as in the <code>fitted.values</code>.</p>
<pre class="r"><code>pred_values &lt;- predict(mod_cwd)
head(pred_values)</code></pre>
<pre><code>##        1        2        3        4        5        6 
## 346.5324 307.3880 692.3082 741.2387 366.1046 920.6506</code></pre>
<pre class="r"><code># pulling out the values using augment
head(augment(mod_cwd)$.fitted)</code></pre>
<pre><code>## [1] 346.5324 307.3880 692.3082 741.2387 366.1046 920.6506</code></pre>
<p>It helps to bind, your predictions (and the standard error) with those from your original data. <strong>NOTE</strong>: that augment already does this for you.</p>
<pre class="r"><code>fitted_vals &lt;- cbind(cwd_data[,c(&quot;RIP.DENS&quot;,&quot;CWD.DENS&quot;)],predict(mod_cwd, se.fit = TRUE))

ggplot(data = fitted_vals) + 
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 2) +
  geom_ribbon(aes(x = RIP.DENS, ymax = fit + 1.96*se.fit, ymin = fit - 1.96*se.fit), fill=&quot;#0E3386&quot;, alpha = 0.5) +
  geom_line(aes(x = RIP.DENS, y = fit), color = &quot;#D12325&quot;, size = 1, linetype = &quot;dashed&quot;) +
  coord_cartesian(ylim = c(0, 1000), expand = FALSE)  +
  scale_y_continuous(breaks = seq(0,1000, by = 250)) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>While these values are helpful in displaying the basic model fit, there are often times (especially when doing multiple linear regression) when you want to look at predictions based off specific values. We can do this by using the <code>newdata</code> in <code>predict()</code>. <strong>NOTE</strong> that the column header names need to reflect the independent values in your model.</p>
<pre class="r"><code>nd &lt;- data.frame(RIP.DENS = 800:2100)

fitted_vals &lt;- cbind(nd,predict(mod_cwd,newdata = nd, se.fit = TRUE))

head(fitted_vals)</code></pre>
<pre><code>##   RIP.DENS      fit   se.fit df residual.scale
## 1      800 39.90112 57.35375 14       150.1832
## 2      801 40.55352 57.28054 14       150.1832
## 3      802 41.20593 57.20739 14       150.1832
## 4      803 41.85834 57.13432 14       150.1832
## 5      804 42.51075 57.06132 14       150.1832
## 6      805 43.16315 56.98838 14       150.1832</code></pre>
<pre class="r"><code>ggplot(data = fitted_vals) + 
  geom_ribbon(aes(x = RIP.DENS, ymax = fit + 1.96*se.fit, ymin = fit - 1.96*se.fit), fill=&quot;#ED174C&quot;, alpha = 0.5) +
  geom_line(aes(x = RIP.DENS, y = fit), color = &quot;#002B5C&quot;, size = 1, linetype = &quot;dashed&quot;) +
  coord_cartesian(ylim = c(0, 1000), expand = FALSE)  +
  scale_y_continuous(breaks = seq(0,1000, by = 250)) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="residuals" class="section level5">
<h5>Residuals</h5>
<p>This difference between each observed \( y_{i} \) and each predicted \(  \) is called a residual \( e_{i} \):</p>
<p><span class="math display">\[ e_i = y_i - \hat{y_i} \]</span></p>
<pre class="r"><code>fitted_vals &lt;- cbind(cwd_data[,c(&quot;RIP.DENS&quot;,&quot;CWD.DENS&quot;)],predict(mod_cwd, se.fit = TRUE))

ggplot(data = fitted_vals) +
    geom_segment(aes(x = RIP.DENS, xend = RIP.DENS,  y = CWD.DENS, yend = fit), linetype = &#39;dotted&#39;, alpha = 0.5) +
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 1, colour =&quot;black&quot;) + 
  geom_point(aes(x = RIP.DENS, y = fit), size = 1, colour =&quot;red&quot;) + 
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="analysis-of-variance" class="section level4">
<h4>Analysis of variance</h4>
<p>In biological sciences we often want to partition the total variation in \( Y \) in part to \( X \) and the other part to the unexplained variation. The partitioned variance is often presented as an analysis of variance (ANOVA) table.</p>
<ul>
<li>Total variation in \( Y \) is the sum of squared deviations of each observation from the sample mean</li>
<li>\( SS_{total} \) has n-1 df and can be partitioned into two additive components</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Variation in \( Y \) explained by \( X \) (difference between \(  \) and \( {y} \). The number of degrees of freedom associated with a linear model is usually the number of parameters minus one.</p></li>
<li><p>Variation in \( Y \) <em>not</em> explained by \( X \) (difference between each observed Y-value and \(  \). Residual (or error) variation. The \( df_{residual} \) is n-2, because we have already estimated \( _0 \) and \( _1 \) to determine the \(  \).</p></li>
</ol>
<ul>
<li>The SS and df are additive</li>
</ul>
<pre class="r"><code>ggplot(data = fitted_vals) +
    geom_segment(data = fitted_vals[fitted_vals$RIP.DENS==1400,],
                 aes(x = RIP.DENS, xend = RIP.DENS,  y = CWD.DENS, yend = fit), linetype = &#39;dotted&#39;) +
  geom_segment(data = fitted_vals[fitted_vals$RIP.DENS==1800,],
                 aes(x = RIP.DENS, xend = RIP.DENS,  y = mean(fitted_vals$CWD.DENS), yend = fit), linetype = &#39;dotted&#39;) +
  geom_point(aes(x = RIP.DENS, y = CWD.DENS), size = 1, colour =&quot;black&quot;) + 
  geom_point(aes(x = RIP.DENS, y = fit), size = 1, colour =&quot;red&quot;) + 
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  annotate(&quot;text&quot;, x = 1800, y = mean(fitted_vals$CWD.DENS), label = &#39; hat(y)[1800]~-~bar(y)&#39;, parse =TRUE) +
  annotate(&quot;text&quot;, x = 1400, y = fitted_vals$CWD.DENS[fitted_vals$RIP.DENS==1400], label = &#39; hat(y)[1400]~-~Y[1400]&#39;, parse =TRUE) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<ul>
<li><p>The \( SS_{total} \) increases with sample size. The Mean SS is a measure of variability that does not depend on sample size. MS is calculated by dividing SS by their df and thus, are not additive.</p></li>
<li><p>The \( MS_{Residual} \) estimates the common variance of the error terms \( e_{i} \), and therefore of the Y-values at each \( x_i \). <strong>NOTE</strong> a key assumption is homogeneity of variances.</p></li>
</ul>
<p>We can calculate the ANOVA table from our linear model in R by using the <code>anova()</code> statment.</p>
<pre class="r"><code>tidy(anova(mod_cwd))</code></pre>
<pre><code>## # A tibble: 2 x 6
##   term         df    sumsq   meansq statistic     p.value
##   &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 RIP.DENS      1 1022447. 1022447.      45.3  0.00000958
## 2 Residuals    14  315770.   22555.      NA   NA</code></pre>
</div>
<div id="variance-explaned-r2-or-r2" class="section level4">
<h4>Variance explaned ( \(r^2\) or \( R^2 \))</h4>
<ul>
<li><p>descriptive measure of association between Y and X (also termed coefficient of variation). the proportion of the total variation in Y that is explained by its linear relationship with X.</p></li>
<li><p>\( 1 =  \)</p></li>
</ul>
</div>
<div id="scatterplot-with-marginal-boxplots" class="section level4">
<h4>Scatterplot with marginal boxplots</h4>
<pre class="r"><code>## Create the base scatterplot

p1 &lt;- ggplot(data = cwd_data) + 
   geom_point(aes(x = RIP.DENS, y = CWD.DENS)) + 
   scale_x_continuous(expand = c(0, 0)) + 
   scale_y_continuous(expand = c(0, 0)) +
   coord_cartesian(xlim = c(700,2300), ylim = c(0,1000)) +
   theme_bw() +
   theme(plot.margin = unit(c(0.2, 0.2, 0.5, 0.5), &quot;lines&quot;))

# horizontal marginal boxplots

p2 &lt;- ggplot(data = cwd_data) + 
   geom_boxplot(aes(x = factor(1),y = RIP.DENS), outlier.colour = &quot;red&quot;) + 
   geom_jitter(aes(x = factor(1),y = RIP.DENS),position = position_jitter(width = 0.05)) + 
   scale_y_continuous(expand = c(0, 0))  +
   coord_flip(ylim = c(700,2300)) + 
   theme_void() +
   theme(axis.text = element_blank(), 
         axis.title = element_blank(), 
         axis.ticks = element_blank(), 
         plot.margin = unit(c(1, 0.2, -0.5, 0.5), &quot;lines&quot;))

# vertical marginal boxplots

p3 &lt;- ggplot(data = cwd_data) + 
   geom_boxplot(aes(x = factor(1),y = CWD.DENS), outlier.colour = &quot;red&quot;) + 
   geom_jitter(aes(x = factor(1),y = CWD.DENS),position = position_jitter(width = 0.05)) + 
   scale_y_continuous(expand = c(0, 0)) +
  coord_cartesian( ylim = c(0,1000)) +
  theme_void() +
   theme(axis.text = element_blank(), 
         axis.title = element_blank(), 
         axis.ticks = element_blank(), 
         plot.margin = unit(c(0.2, 1, 0.5, -0.5), &quot;lines&quot;))</code></pre>
<pre class="r"><code>gt1 &lt;- ggplot_gtable(ggplot_build(p1))
gt2 &lt;- ggplot_gtable(ggplot_build(p2))
gt3 &lt;- ggplot_gtable(ggplot_build(p3))</code></pre>
<pre class="r"><code># Get maximum widths and heights
maxWidth &lt;- unit.pmax(gt1$widths[2:3], gt2$widths[2:3])
maxHeight &lt;- unit.pmax(gt1$heights[4:5], gt3$heights[4:5])

# Set the maximums in the gtables for gt1, gt2 and gt3
gt1$widths[2:3] &lt;- as.list(maxWidth)
gt2$widths[2:3] &lt;- as.list(maxWidth)

gt1$heights[4:5] &lt;- as.list(maxHeight)
gt3$heights[4:5] &lt;- as.list(maxHeight)</code></pre>
<pre class="r"><code># Create a new gtable
gt &lt;- gtable(widths = unit(c(7, 1), &quot;null&quot;), height = unit(c(1, 7), &quot;null&quot;))

# Insert gt1, gt2 and gt3 into the new gtable
gt &lt;- gtable_add_grob(gt, gt1, 2, 1)
gt &lt;- gtable_add_grob(gt, gt2, 1, 1)
gt &lt;- gtable_add_grob(gt, gt3, 2, 2)


# And render the plot
grid.newpage()
grid.draw(gt)</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code># grid.rect(x = 0.5, y = 0.5, height = 0.995, width = 0.995, default.units = &quot;npc&quot;, 
#     gp = gpar(col = &quot;black&quot;, fill = NA, lwd = 1))</code></pre>
</div>
<div id="assumptions-of-a-regression-model" class="section level4">
<h4>Assumptions of a regression model</h4>
<ol style="list-style-type: decimal">
<li>Normality (except GLMs)</li>
<li>Homogeneity of variance</li>
<li>Independence</li>
<li>Fixed X</li>
</ol>
</div>
<div id="regression-diagnostics" class="section level4">
<h4>Regression diagnostics</h4>
<ul>
<li>A proper interpretation of a linear regression analysis should also include checks of how well the model fits the observed data</li>
</ul>
<ol style="list-style-type: decimal">
<li>Is a straight line appropriate?</li>
<li>Influence of outliers?</li>
</ol>
<ul>
<li>See-saw, balanced on the mean of X</li>
</ul>
<div id="leverage" class="section level5">
<h5>Leverage</h5>
<ul>
<li><p>Leverage is a measure of how extreme an observation is for the \(X \)-variable</p></li>
<li><p>Generally concerned when a value is 2 or 3 times greater than the mean value</p></li>
</ul>
<pre class="r"><code>fitted_vals2 &lt;- cbind(cwd_data[,c(&quot;RIP.DENS&quot;,&quot;CWD.DENS&quot;)],predict(mod_cwd, se.fit = TRUE))

mod_hat&lt;-hatvalues(mod_cwd)
mean_hat &lt;- mean(mod_hat)

fitted_vals2$resid.out &lt;- 0

fitted_vals2$resid.out[which(mod_hat &gt; 2*mean_hat)] &lt;- 1

fitted_vals2$resid.out &lt;- as.factor(fitted_vals2$resid.out )

ggplot(data = fitted_vals2)  +
  geom_point(aes(x = RIP.DENS, y = CWD.DENS, colour = resid.out), size = 2) + 
  geom_line(aes(x = RIP.DENS, y = fit), color = &quot;#002B5C&quot;, size = 1, linetype = &quot;dashed&quot;) +
  coord_cartesian(ylim = c(0, 1000), xlim = c(750, 2200), expand = FALSE)  +
  scale_y_continuous(breaks = seq(0,1000, by = 250)) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  scale_colour_manual(values = c(&quot;0&quot; = &quot;red&quot;, &quot;1&quot; = &quot;blue&quot;)) +
  theme_bw() + 
  theme(legend.position = c(0.90, 0.25))</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="residuals-1" class="section level5">
<h5>Residuals</h5>
<ul>
<li><p>Residuals are an important way of checking regression assumptions</p></li>
<li><p>Studentized residuals do have constant variance so different studentized residuals can be compared</p></li>
</ul>
<pre class="r"><code>std_res &lt;- studres(mod_cwd)

fitted_vals2$resid.std &lt;- std_res

ggplot(data = fitted_vals2)  +
  geom_point(aes(x = RIP.DENS, y = resid.std), size = 2) + 
  coord_cartesian(ylim = c(-4, 4), xlim = c(750, 2200), expand = FALSE)  +
  scale_y_continuous(breaks = seq(-4,4, by = 1)) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  scale_colour_manual(values = c(&quot;0&quot; = &quot;red&quot;, &quot;1&quot; = &quot;blue&quot;)) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>#outlier.test in car package
outlierTest(mod_cwd)</code></pre>
<pre><code>## No Studentized residuals with Bonferonni p &lt; 0.05
## Largest |rstudent|:
##    rstudent unadjusted p-value Bonferonni p
## 6 -3.104947          0.0083667      0.13387</code></pre>
</div>
<div id="influence" class="section level5">
<h5>Influence</h5>
<ul>
<li><p>Cook’s distance statistic, \( D_i \), is the measure of the influence each observation has on the fitted regression line and the estimates of the regression parameters.</p></li>
<li><p>A large \( D_i \) indicates that removal of that observation would change the estimates of the regression parameters considerably</p></li>
</ul>
<pre class="r"><code>fitted_vals3&lt;-augment(mod_cwd)

ggplot(data = fitted_vals3)  +
  geom_point(aes(x = RIP.DENS, y = CWD.DENS, colour = .cooksd ), size = 2) + 
  coord_cartesian(ylim = c(0, 1000), xlim = c(750, 2200), expand = FALSE)  +
  scale_y_continuous(breaks = seq(0,1000, by = 250)) +
  labs(x = &quot;Riparian tree density&quot;, y =&quot;Coarse woody debris density&quot;) +
  scale_colour_continuous(low = &quot;black&quot;, high = &quot;red&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
</div>
<div id="weighted-least-squares" class="section level4">
<h4>Weighted least squares</h4>
<ul>
<li>Responses are averages with known sample sizes</li>
<li>Responses are estimates and SEs are available
<ul>
<li>\( w_i =  \)</li>
</ul></li>
<li>Variance is proportional to X
<ul>
<li>\( w_i =  \) or \( w_i =  \)</li>
</ul></li>
</ul>
<pre class="r"><code>set.seed(12345)

x = rnorm(100,0,3)
y = 3-2*x + rnorm(100,0,sapply(x,function(x){1+0.5*x^2}))

fake_data1 &lt;- data.frame(x = x, y = y)

mod_1 &lt;- lm(y ~ x, data = fake_data1)
summary(mod_1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = fake_data1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.1365  -1.9392   0.4543   3.2609  27.8710 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   2.5684     0.8601   2.986     0.00357 ** 
## x            -1.3402     0.2524  -5.310 0.000000685 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.398 on 98 degrees of freedom
## Multiple R-squared:  0.2234, Adjusted R-squared:  0.2155 
## F-statistic: 28.19 on 1 and 98 DF,  p-value: 0.000000685</code></pre>
<pre class="r"><code>fake_pred&lt;-cbind(fake_data1, predict(mod_1, se.fit = TRUE))

ggplot(data = fake_pred) +
  geom_point(aes(x = x, y = y), size = 2, colour = &quot;red&quot;) +
  geom_line(aes(x = x, y = fit), linetype = &#39;dashed&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>mod_2 &lt;- lm(y ~ x, weights= 1/(x^2), data = fake_data1)
summary(mod_2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = fake_data1, weights = 1/(x^2))
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.4230 -1.0989  0.1181  1.4912  4.4567 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.7659     0.1213  22.794  &lt; 2e-16 ***
## x            -1.6554     0.2092  -7.911 3.92e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.07 on 98 degrees of freedom
## Multiple R-squared:  0.3898, Adjusted R-squared:  0.3835 
## F-statistic: 62.59 on 1 and 98 DF,  p-value: 3.918e-12</code></pre>
<pre class="r"><code>summary(mod_1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = fake_data1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.1365  -1.9392   0.4543   3.2609  27.8710 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   2.5684     0.8601   2.986     0.00357 ** 
## x            -1.3402     0.2524  -5.310 0.000000685 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.398 on 98 degrees of freedom
## Multiple R-squared:  0.2234, Adjusted R-squared:  0.2155 
## F-statistic: 28.19 on 1 and 98 DF,  p-value: 0.000000685</code></pre>
<pre class="r"><code>fake_pred2&lt;-cbind(fake_data1, predict(mod_2, se.fit = TRUE))

ggplot(data = fake_pred) +
  geom_point(aes(x = x, y = y), size = 2, colour = &quot;red&quot;) +
  geom_line(aes(x = x, y = fit), linetype = &#39;dashed&#39;) +
  geom_line(data =fake_pred2, aes(x = x, y = fit), linetype = &#39;solid&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
</div>
<div id="random-x-model-ii-regression" class="section level4">
<h4>Random X (Model II Regression)</h4>
<ul>
<li>Both \(X \) and \(Y \) chosen haphazardly or at random</li>
<li>Model II Regression and the approach is controversial</li>
<li>If the purpose of regression is <strong>prediction</strong>, then OLS</li>
<li>If the purpose of regression is <strong>mechanisms</strong>, then <strong>not</strong> OLS (?)
<ul>
<li>error variability associated with both Y \( <em>{}^2 \) and X \( </em>{}^2 \) and the OLS estimate of \( _1 \) is biased towards zero</li>
</ul></li>
<li>Major axis (MA) regression fits line minimizing the sum of squared perpendicular distances from each observation to the fitted line
<ul>
<li>\( <em>{}^2 \) = \( </em>{}^2 \)</li>
</ul></li>
<li>Reduced major axis (RMA) regression or standard major axis (SMA) regression is fitted by minimizing the sum of areas of the triangles formed by vertical and horizontal lines from each observation to the fitted line</li>
<li>\( _{}^2 <em>x^2 \) and \( </em>{}^2 _y^2\)</li>
</ul>
<pre class="r"><code> # install.packages(&#39;lmodel2&#39;)
 
 data(mod2ex2)
 
Ex2.res &lt;- lmodel2(Prey ~ Predators, data=mod2ex2, &quot;relative&quot;, &quot;relative&quot;, 99)
Ex2.res</code></pre>
<pre><code>## 
## Model II regression
## 
## Call: lmodel2(formula = Prey ~ Predators, data = mod2ex2, range.y
## = &quot;relative&quot;, range.x = &quot;relative&quot;, nperm = 99)
## 
## n = 20   r = 0.8600787   r-square = 0.7397354 
## Parametric P-values:   2-tailed = 0.000001161748    1-tailed = 0.0000005808741 
## Angle between the two OLS regression lines = 5.106227 degrees
## 
## Permutation tests of OLS, MA, RMA slopes: 1-tailed, tail corresponding to sign
## A permutation test of r is equivalent to a permutation test of the OLS slope
## P-perm for SMA = NA because the SMA slope cannot be tested
## 
## Regression results
##   Method Intercept    Slope Angle (degrees) P-perm (1-tailed)
## 1    OLS  20.02675 2.631527        69.19283              0.01
## 2     MA  13.05968 3.465907        73.90584              0.01
## 3    SMA  16.45205 3.059635        71.90073                NA
## 4    RMA  17.25651 2.963292        71.35239              0.01
## 
## Confidence intervals
##   Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope
## 1    OLS      12.490993        27.56251   1.858578    3.404476
## 2     MA       1.347422        19.76310   2.663101    4.868572
## 3    SMA       9.195287        22.10353   2.382810    3.928708
## 4    RMA       8.962997        23.84493   2.174260    3.956527
## 
## Eigenvalues: 269.8212 6.418234 
## 
## H statistic used for computing C.I. of MA: 0.006120651</code></pre>
<ul>
<li>Simulated comparisons of OLS, MA and RMA regression analyses when X is random indicated:</li>
</ul>
<ol style="list-style-type: decimal">
<li>RMA estimate of \( /beta_1 )\ is less biased than the MA estimate</li>
<li>If the error variability in X is more than ~ a third of the error variability in Y, then RMA is the preferred method; otherwise OLS is acceptable</li>
</ol>
</div>
<div id="robust-regression" class="section level4">
<h4>Robust regression</h4>
<ul>
<li>Limitation of OLS is that the estimates of model parameters, and therefore subsequent hypothesis tests, can be sensitive to distributional assumptions and affected by outlying observations</li>
</ul>
<div id="least-absolute-deviance-lad" class="section level5">
<h5>Least absolute deviance (LAD)</h5>
<ul>
<li>Minimize the sum of absolute values of the residuals rather than the sum of squared residuals
<ul>
<li>not squaring the residuals, extreme observations have less influence on the fitted model</li>
</ul></li>
</ul>
</div>
<div id="m-estimator" class="section level5">
<h5>M-estimator</h5>
<ul>
<li>M-estimators involve minimizing the sum of some function of \( e_i \)</li>
<li>Huber M-estimators, <a href="https://chrischizinski.github.io/SNR_R_Group/2016-10-07-REstimation">described earlier</a>, weight the observations differently depending how far they are from the central tendency</li>
</ul>
<pre class="r"><code>crime_data &lt;- read_csv(getURL(&quot;https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/crime_data.csv&quot;))
head(crime_data)</code></pre>
<pre><code>## # A tibble: 6 x 9
##     sid state crime murder pctmetro pctwhite pcths poverty single
##   &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1     1 ak      761   9        41.8     75.2  86.6    9.10   14.3
## 2     2 al      780  11.6      67.4     73.5  66.9   17.4    11.5
## 3     3 ar      593  10.2      44.7     82.9  66.3   20      10.7
## 4     4 az      715   8.60     84.7     88.6  78.7   15.4    12.1
## 5     5 ca     1078  13.1      96.7     79.3  76.2   18.2    12.5
## 6     6 co      567   5.80     81.8     92.5  84.4    9.90   12.1</code></pre>
<pre class="r"><code>crime_mod &lt;- lm(crime ~ single, data = crime_data)
summary(crime_mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crime ~ single, data = crime_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -767.42 -116.82  -20.58  125.28  719.70 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1362.53     186.23  -7.316 2.15e-09 ***
## single        174.42      16.17  10.788 1.53e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 242.5 on 49 degrees of freedom
## Multiple R-squared:  0.7037, Adjusted R-squared:  0.6977 
## F-statistic: 116.4 on 1 and 49 DF,  p-value: 1.529e-14</code></pre>
<pre class="r"><code>crime_fit &lt;- data.frame(state = crime_data$state, augment(crime_mod))</code></pre>
<pre class="r"><code>ggplot(data = crime_fit) +
  geom_line(aes(x = single,  y = .fitted), color = &quot;red&quot;) +
  geom_point(aes(x = single,  y = crime, colour = .std.resid ), size = 2) +
  geom_text(aes(x = single,  y = crime, label = state ), hjust = 0, vjust = 1) +
  scale_colour_continuous(low = &quot;red&quot;, high = &quot;blue&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>mod_huber &lt;- rlm(crime ~ single, data = crime_data)
summary(mod_huber)</code></pre>
<pre><code>## 
## Call: rlm(formula = crime ~ single, data = crime_data)
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -797.489 -130.956   -9.108  127.019  716.664 
## 
## Coefficients:
##             Value      Std. Error t value   
## (Intercept) -1429.3999   164.1623    -8.7072
## single        181.0128    14.2519    12.7010
## 
## Residual standard error: 192.9 on 49 degrees of freedom</code></pre>
<pre class="r"><code>huber_dat&lt;-data.frame(state = crime_data$state, augment(mod_huber), weight = mod_huber$w)</code></pre>
<pre class="r"><code>ggplot(data = huber_dat) +
  geom_line(aes(x = single,  y = .fitted), color = &quot;red&quot;) +
  geom_point(aes(x = single,  y = crime, colour = weight ), size = 2) +
  geom_text(aes(x = single,  y = crime, label = state ), hjust = 0, vjust = 1) +
  scale_colour_continuous(low = &quot;red&quot;, high = &quot;blue&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2016-10-28-SimpleRegression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
</div>
<div id="rank-based-regression" class="section level4">
<h4>Rank-based regression</h4>
<ul>
<li>Does not assume any specific distribution of the error terms but still fits the usual linear regression model
<ul>
<li>Transformations are either ineffective or misrepresent the underlying biological process</li>
</ul></li>
</ul>
<pre class="r"><code># install.packages(&quot;Rfit&quot;)

mod_rank &lt;- rfit(crime ~ single, data = crime_data)
summary(mod_rank)</code></pre>
<pre><code>## Call:
## rfit.default(formula = crime ~ single, data = crime_data)
## 
## Coefficients:
##              Estimate Std. Error t.value   p.value    
## (Intercept) -1400.923    182.905 -7.6593 6.362e-10 ***
## single        176.538     15.861 11.1305 5.109e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## Overall Wald Test: 123.8871 p-value: 0</code></pre>
</div>
</div>
<div id="relationship-between-regression-and-correlation" class="section level3">
<h3>Relationship between regression and correlation</h3>
<ul>
<li>Simple correlation analysis is used when we seek to measure the strength of the linear relationship (the correlation coefficient) between the two variables</li>
<li>Regression analysis is used when we can biologically distinguish a response \(( Y \) to a predictor variable \( X \)
<ul>
<li>We can construct a model relating \(( Y \) to \( X \) and this to predict \(( Y \) from \( X \)</li>
</ul></li>
</ul>
</div>
</div>
</div>

    
    

    

        <h4 class="page-header">Related</h4>

         <div class="item">

    
    
    

    
    

    <h4><a href="/post/2016-12-16-anova_multifactor/">Multiple factor ANOVA</a></h4>
    <h5>December 16, 2016</h5>
    
    <a href="/tags/regression"><kbd class="item-tag">Regression</kbd></a>
    
    <a href="/tags/anova"><kbd class="item-tag">ANOVA</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4><a href="/post/2016-12-09-anova_1factor/">ANOVA</a></h4>
    <h5>December 9, 2016</h5>
    
    <a href="/tags/regression"><kbd class="item-tag">Regression</kbd></a>
    
    <a href="/tags/anova"><kbd class="item-tag">ANOVA</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4><a href="/post/2016-11-11-multipleregression/">Multiple Regression</a></h4>
    <h5>November 11, 2016</h5>
    
    <a href="/tags/regression"><kbd class="item-tag">Regression</kbd></a>
    

</div>
 

    

    

        <h4 class="page-header">Comments</h4>

        <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "snrrgroup" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    

</main>

        <footer>

            <p class="copyright text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a></p>

        </footer>
       
    </body>

</html>

