---
title: 'Applied Multivariate:  Breaking multivariate data into groups. Part 2.'
date: '2017-10-06'
categories: ["R"]
tags: ["Applied Multivariate"]
---



<pre class="r"><code>library(tidyverse)
library(cluster)
library(vegan)
library(factoextra)
library(fpc)
library(RWeka)
library(ggdendro)
library(NbClust)</code></pre>
<div id="first-lets-load-the-data-from-last-week" class="section level2">
<h2>First lets load the data from last week</h2>
<pre class="r"><code>data(&quot;USArrests&quot;)</code></pre>
<p>Lets scale the data</p>
<pre class="r"><code>USArrests %&gt;% 
  scale() -&gt; arrest.scale

head(arrest.scale)</code></pre>
<pre><code>##                Murder   Assault   UrbanPop         Rape
## Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
## Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
## Arizona    0.07163341 1.4788032  0.9989801  1.042878388
## Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
## California 0.27826823 1.2628144  1.7589234  2.067820292
## Colorado   0.02571456 0.3988593  0.8608085  1.864967207</code></pre>
<p>lets convert this to a distance matrix using the <code>factoextra::get_dist()</code> function.</p>
<pre class="r"><code>arrest.scale %&gt;% 
  get_dist(upper = TRUE, diag = TRUE) -&gt; arrest.dist</code></pre>
</div>
<div id="cluster-analysis" class="section level2">
<h2>Cluster analysis</h2>
<div id="partitioning-clustering" class="section level3">
<h3>Partitioning clustering</h3>
<div id="k-means" class="section level4">
<h4>K-means</h4>
<pre class="r"><code>km.arrest &lt;- kmeans(arrest.scale, centers = 3, nstart = 25)
km.arrest</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 17, 13, 20
## 
## Cluster means:
##       Murder    Assault   UrbanPop       Rape
## 1 -0.4469795 -0.3465138  0.4788049 -0.2571398
## 2 -0.9615407 -1.1066010 -0.9301069 -0.9667633
## 3  1.0049340  1.0138274  0.1975853  0.8469650
## 
## Clustering vector:
##        Alabama         Alaska        Arizona       Arkansas     California 
##              3              3              3              1              3 
##       Colorado    Connecticut       Delaware        Florida        Georgia 
##              3              1              1              3              3 
##         Hawaii          Idaho       Illinois        Indiana           Iowa 
##              1              2              3              1              2 
##         Kansas       Kentucky      Louisiana          Maine       Maryland 
##              1              2              3              2              3 
##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
##              1              3              2              3              3 
##        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
##              2              2              3              2              1 
##     New Mexico       New York North Carolina   North Dakota           Ohio 
##              3              3              3              2              1 
##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
##              1              1              1              1              3 
##   South Dakota      Tennessee          Texas           Utah        Vermont 
##              2              3              3              1              2 
##       Virginia     Washington  West Virginia      Wisconsin        Wyoming 
##              1              1              2              2              1 
## 
## Within cluster sum of squares by cluster:
## [1] 19.62285 11.95246 46.74796
##  (between_SS / total_SS =  60.0 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
<pre class="r"><code>centers&lt;-as.data.frame(km.arrest$centers)
centers$cluster &lt;- rownames(centers)

centers %&gt;% 
  gather(type, value, -cluster) %&gt;% 
  ggplot() + 
  geom_bar(aes(x = type, y = value, fill = type), position = &quot;dodge&quot;, stat = &quot;identity&quot;, colour = &quot;black&quot;) +
  facet_wrap(~cluster) +
  theme_classic() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>So how would we describe these clusters?</p>
<ul>
<li>Cluster 1: Lower than average crime, above average urban population</li>
<li>Cluster 2: Higher than average crime, and average urban population</li>
<li>Cluster 3: Much lower than average crime with below average urban population</li>
</ul>
<p>We can get the exact stats from our scaled data</p>
<pre class="r"><code>attr(arrest.scale,&quot;scaled:center&quot;)</code></pre>
<pre><code>##   Murder  Assault UrbanPop     Rape 
##    7.788  170.760   65.540   21.232</code></pre>
<pre class="r"><code>attr(arrest.scale,&quot;scaled:scale&quot;)</code></pre>
<pre><code>##    Murder   Assault  UrbanPop      Rape 
##  4.355510 83.337661 14.474763  9.366385</code></pre>
<p>Lets visualize these in ordination space.</p>
<p>We can use the <code>factoextra::fviz_cluster</code> to illustrate how these places fall out</p>
<pre class="r"><code>fviz_cluster(km.arrest, data = arrest.scale,
             ellipse.type = &quot;convex&quot;,
             palette = &quot;jco&quot;,
             ggtheme = theme_minimal())</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><code>factoextra::fviz_cluster</code> while quick can be clunky and difficult to get exactly how you would like your plot displayed. Here is how you can break the data apart and plot yourself.</p>
<pre class="r"><code>trythis&lt;-stats::prcomp(arrest.scale, scale = FALSE, center = FALSE)
state_scores&lt;-as.data.frame(scores(trythis))
state_scores$cluster &lt;- km.arrest$cluster
state_scores$state &lt;- rownames(state_scores)
head(state_scores)</code></pre>
<pre><code>##                   PC1        PC2         PC3          PC4 cluster
## Alabama    -0.9756604  1.1220012 -0.43980366  0.154696581       3
## Alaska     -1.9305379  1.0624269  2.01950027 -0.434175454       3
## Arizona    -1.7454429 -0.7384595  0.05423025 -0.826264240       3
## Arkansas    0.1399989  1.1085423  0.11342217 -0.180973554       1
## California -2.4986128 -1.5274267  0.59254100 -0.338559240       3
## Colorado   -1.4993407 -0.9776297  1.08400162  0.001450164       3
##                 state
## Alabama       Alabama
## Alaska         Alaska
## Arizona       Arizona
## Arkansas     Arkansas
## California California
## Colorado     Colorado</code></pre>
<p>Next we need to find which points fall on the outside of each group (i.e., hull). We do this using the <code>chull()</code> command.</p>
<pre class="r"><code>chull(state_scores %&gt;% filter(cluster ==1) %&gt;% select(PC1, PC2) )</code></pre>
<pre><code>## [1]  2  4  8  3  1 17</code></pre>
<pre class="r"><code>grp.1 &lt;- state_scores[state_scores$cluster == 1, ][chull(state_scores %&gt;% filter(cluster ==1) %&gt;% select(PC1, PC2) ), ]  # hull values for cluster 1

grp.2 &lt;- state_scores[state_scores$cluster == 2, ][chull(state_scores %&gt;% filter(cluster ==2) %&gt;% select(PC1, PC2) ), ]  # hull values for cluster 2

grp.3 &lt;- state_scores[state_scores$cluster == 3, ][chull(state_scores %&gt;% filter(cluster ==3) %&gt;% select(PC1, PC2) ), ]  # hull values for cluster 3

all_hulls &lt;- rbind(grp.1,grp.2,grp.3)
head(all_hulls)</code></pre>
<pre><code>##                     PC1        PC2         PC3        PC4 cluster
## Connecticut  1.34499236 -1.0779836 -0.63679250 -0.1172787       1
## Hawaii       0.90348448 -1.5546761  0.05027151  0.8937332       1
## New Jersey  -0.17974128 -1.4349374 -0.75677041  0.2409366       1
## Delaware    -0.04722981 -0.3220889 -0.71141032 -0.8731133       1
## Arkansas     0.13999894  1.1085423  0.11342217 -0.1809736       1
## Wyoming      0.62310061  0.3177866 -0.23824049 -0.1649769       1
##                   state
## Connecticut Connecticut
## Hawaii           Hawaii
## New Jersey   New Jersey
## Delaware       Delaware
## Arkansas       Arkansas
## Wyoming         Wyoming</code></pre>
<pre class="r"><code>ggplot(data = state_scores) + 
  geom_point(aes(x = PC1, y = PC2, color = as.factor(cluster))) +
  geom_text(aes(x = PC1, y = PC2, color = as.factor(cluster), label = state))  +
  geom_polygon(data = all_hulls, aes(x = PC1, y = PC2, fill = as.factor(cluster), colour =  as.factor(cluster)), alpha = 0.25) + 
  theme_minimal() </code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Above we chose to seperate the data into three clusters. But how do we decide what is the appropriate number of clusters?</p>
<p>There are four different ways this can be done:</p>
<ol style="list-style-type: decimal">
<li><p>Cross validation. A subset of the data is used to develop the model and then it is ‘verfied’ with the rest of the data by checking the sum of squared distances to the group centroids. An average of the sum of square distances is then taken. Best number of clusters should have the lowest average squared distance.</p></li>
<li><p>Elbow method. Similar to a scree plot where you choose the “elbow” in the plot representing a decrease in the rate of change in variance</p></li>
</ol>
<pre class="r"><code>wss &lt;- (nrow(arrest.scale)-1)*sum(apply(arrest.scale,2,var))

nclusters = 15

for(i in 2: nclusters){
  wss[i]&lt;-sum(kmeans(arrest.scale, centers = i, nstart = 25)$withinss)
  
}

scree_data&lt;-data.frame(wss = wss, clusters = 1:nclusters)

head(scree_data)</code></pre>
<pre><code>##         wss clusters
## 1 196.00000        1
## 2 102.86240        2
## 3  78.32327        3
## 4  56.40317        4
## 5  48.94420        5
## 6  42.87954        6</code></pre>
<p>And plot out the scree plot</p>
<pre class="r"><code>ggplot(data = scree_data) + 
  geom_line(aes(x = clusters, y = wss)) +
  geom_point(aes(x = clusters, y = wss), size = 3) +
  scale_x_continuous(breaks = 1:nclusters) +
  theme_classic()</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The elbow in this plot is at 4, but you can see that this can be a little tricky in finding the “elbow”</p>
<ol start="3" style="list-style-type: decimal">
<li>Silouette method, returns a value -1 to 1 based on the similarity of an observatioin within its own cluster and compared across clusters. A high value would indicate a strong match.</li>
</ol>
<pre class="r"><code>fviz_nbclust(arrest.scale, kmeans,
             method = &quot;silhouette&quot;)</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>There is also a type of clustering, that clusters around a mediod rather than a centroid. In using, PAM clustering each cluster is represented by one of the objects in the cluster. PAM is less sensitive to outliers compared to k-means</p>
<pre class="r"><code>pamkout&lt;-fpc::pamk(arrest.scale, krange = 2:15)
pamkout</code></pre>
<pre><code>## $pamobject
## Medoids:
##            ID     Murder    Assault   UrbanPop       Rape
## New Mexico 31  0.8292944  1.3708088  0.3081225  1.1603196
## Nebraska   27 -0.8008247 -0.8250772 -0.2445636 -0.5052109
## Clustering vector:
##        Alabama         Alaska        Arizona       Arkansas     California 
##              1              1              1              2              1 
##       Colorado    Connecticut       Delaware        Florida        Georgia 
##              1              2              2              1              1 
##         Hawaii          Idaho       Illinois        Indiana           Iowa 
##              2              2              1              2              2 
##         Kansas       Kentucky      Louisiana          Maine       Maryland 
##              2              2              1              2              1 
##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
##              2              1              2              1              1 
##        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
##              2              2              1              2              2 
##     New Mexico       New York North Carolina   North Dakota           Ohio 
##              1              1              1              2              2 
##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
##              2              2              2              2              1 
##   South Dakota      Tennessee          Texas           Utah        Vermont 
##              2              1              1              2              2 
##       Virginia     Washington  West Virginia      Wisconsin        Wyoming 
##              2              2              2              2              2 
## Objective function:
##    build     swap 
## 1.441358 1.368969 
## 
## Available components:
##  [1] &quot;medoids&quot;    &quot;id.med&quot;     &quot;clustering&quot; &quot;objective&quot;  &quot;isolation&quot; 
##  [6] &quot;clusinfo&quot;   &quot;silinfo&quot;    &quot;diss&quot;       &quot;call&quot;       &quot;data&quot;      
## 
## $nc
## [1] 2
## 
## $crit
##  [1] 0.0000000 0.4084890 0.3143656 0.3389904 0.3105170 0.2629987 0.2243815
##  [8] 0.2386072 0.2466113 0.2447023 0.2525199 0.2548601 0.2514600 0.2504588
## [15] 0.2400069</code></pre>
<pre class="r"><code>medoid_data &lt;- data.frame(pamkout$pamobject$medoids)
medoid_data$state&lt;-rownames(medoid_data)

medoid_data %&gt;% 
  gather(type, value, -state) %&gt;% 
  ggplot() + 
  geom_bar(aes(x = type, y = value, fill = type), position = &quot;dodge&quot;, stat = &quot;identity&quot;, colour = &quot;black&quot;) +
  facet_wrap(~state) +
  theme_classic() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<ol start="4" style="list-style-type: decimal">
<li>X means clustering. Similary to the k means clustering but uses Bayesian Information criteria to identify the best split.</li>
</ol>
<pre class="r"><code>WPM(&quot;refresh-cache&quot;)
# WPM(&quot;list-packages&quot;, &quot;available&quot;)
WPM(&quot;install-package&quot;, &quot;XMeans&quot;)

xout&lt;-XMeans(arrest.scale)
xout

xout$class_ids</code></pre>
<pre class="r"><code>xcenters&lt;-data.frame(cluster = 1:2,matrix(c(1.004934034580132, 1.0138273518643042,0.19758526759992892,0.8469650134151087,-0.6699560230534215, -0.675884901242869,-0.1317235117332867,-0.564643342276739), byrow = TRUE, ncol = 4))
names(xcenters)[2:5]&lt;- colnames(arrest.scale)

xcenters %&gt;% 
  gather(type, value, -cluster) %&gt;% 
  ggplot() + 
  geom_bar(aes(x = type, y = value, fill = type), position = &quot;dodge&quot;, stat = &quot;identity&quot;, colour = &quot;black&quot;) +
  facet_wrap(~cluster) +
  theme_classic() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>There is another type of partitioning clustering, called clara and is used primarily for large datasets. We will not be discussing in this class, but there is a function <code>cluster::clara()</code> (among some others) to be used when clustering “big data”.</p>
</div>
</div>
<div id="hierarchical-clustering" class="section level3">
<h3>Hierarchical clustering</h3>
<p>Two main types of hierarchical clustering</p>
<ol style="list-style-type: decimal">
<li>Agglomerative clustering. Treats each observation as own cluster and then begins to merge into new clusters until all are merged into a new cluster. Upside down tree. This is the most common type of hierarchical clustering.</li>
</ol>
<pre class="r"><code>USArrests %&gt;% 
  scale() %&gt;% 
  dist(method = &quot;euclidian&quot;) -&gt; arrest.euc

arrest_clust&lt;-hclust(arrest.euc, method = &quot;ward.D2&quot;)

arrest_clust</code></pre>
<pre><code>## 
## Call:
## hclust(d = arrest.euc, method = &quot;ward.D2&quot;)
## 
## Cluster method   : ward.D2 
## Distance         : euclidean 
## Number of objects: 50</code></pre>
<p>We have several options to plot our dendrograms.</p>
<pre class="r"><code>plot(arrest_clust)</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>fviz_dend(arrest_clust, 
          k = 2, # two groups
          cex = 0.5) # label size</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>ggdendrogram(arrest_clust,rotate=TRUE, size = 1)</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Or extract the data and build your own plot</p>
<pre class="r"><code>arrest_dendro  &lt;- as.dendrogram(arrest_clust)

dend_dendro_data &lt;- dendro_data(arrest_dendro, type = &quot;rectangle&quot;)

names(dend_dendro_data)</code></pre>
<pre><code>## [1] &quot;segments&quot;    &quot;labels&quot;      &quot;leaf_labels&quot; &quot;class&quot;</code></pre>
<pre class="r"><code>head( dend_dendro_data$segments)</code></pre>
<pre><code>##           x         y     xend      yend
## 1 19.771484 13.516242 8.867188 13.516242
## 2  8.867188 13.516242 8.867188  6.461866
## 3  8.867188  6.461866 4.125000  6.461866
## 4  4.125000  6.461866 4.125000  2.714554
## 5  4.125000  2.714554 2.500000  2.714554
## 6  2.500000  2.714554 2.500000  1.091092</code></pre>
<pre class="r"><code>ggplot(data = dend_dendro_data$segments)+
  geom_segment(aes(x = x, y = y, xend = xend, yend=yend), size =1, colour = &quot;red&quot;) + 
  theme_void()</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>head( dend_dendro_data$labels)</code></pre>
<pre><code>##   x y          label
## 1 1 0        Alabama
## 2 2 0      Louisiana
## 3 3 0        Georgia
## 4 4 0      Tennessee
## 5 5 0 North Carolina
## 6 6 0    Mississippi</code></pre>
<pre class="r"><code>ggplot(data = dend_dendro_data$segments)+
  geom_segment(aes(x = x, y = y, xend = xend, yend=yend), size =1, colour = &quot;red&quot;) + 
  geom_text(data = dend_dendro_data$labels, aes(x = x, y = y, label = label), colour = &quot;blue&quot;, hjust = 1, angle = 90, size =3) +
  coord_cartesian(ylim=c(-3,15)) +
  theme_void()</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<p>Choosing the appropriate number of clusters</p>
<pre class="r"><code>NbClust(arrest.scale,
        distance = &quot;euclidean&quot;,
        min.nc = 2, max.nc = 10,
        method = &quot;complete&quot;, index =&quot;all&quot;) -&gt; arrest.nb</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 9 proposed 2 as the best number of clusters 
## * 4 proposed 3 as the best number of clusters 
## * 6 proposed 4 as the best number of clusters 
## * 2 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
<pre class="r"><code>fviz_nbclust(arrest.nb, ggtheme = theme_minimal())</code></pre>
<pre><code>## Among all indices: 
## ===================
## * 2 proposed  0 as the best number of clusters
## * 1 proposed  1 as the best number of clusters
## * 9 proposed  2 as the best number of clusters
## * 4 proposed  3 as the best number of clusters
## * 6 proposed  4 as the best number of clusters
## * 2 proposed  5 as the best number of clusters
## * 1 proposed  8 as the best number of clusters
## * 1 proposed  10 as the best number of clusters
## 
## Conclusion
## =========================
## * According to the majority rule, the best number of clusters is  2 .</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Divisive clustering. Start with one cluster and then begin breaking apart into more similar clusters.</li>
</ol>
<pre class="r"><code>res.diana &lt;- diana(USArrests, stand = TRUE)
res.diana</code></pre>
<pre><code>## Merge:
##       [,1] [,2]
##  [1,]  -15  -29
##  [2,]  -13  -32
##  [3,]  -23  -49
##  [4,]  -14  -36
##  [5,]  -20  -31
##  [6,]  -16  -38
##  [7,]  -37  -47
##  [8,]    1  -19
##  [9,]    4  -35
## [10,]  -41  -48
## [11,]  -46  -50
## [12,]  -12  -27
## [13,]  -21  -30
## [14,]  -24  -40
## [15,]    2  -43
## [16,]  -17  -26
## [17,]   -1  -42
## [18,]  -10  -18
## [19,]    9    6
## [20,]   -4   11
## [21,]  -11  -44
## [22,]   -7  -39
## [23,]    8  -34
## [24,]   10  -45
## [25,]    5  -22
## [26,]   17   18
## [27,]   14  -33
## [28,]   12    3
## [29,]   -5  -28
## [30,]   -3   25
## [31,]   29   -6
## [32,]   15  -25
## [33,]   30   32
## [34,]   22   13
## [35,]   23   24
## [36,]   19    7
## [37,]   20   -8
## [38,]   34   21
## [39,]   28   16
## [40,]   37   36
## [41,]   26   27
## [42,]   39   35
## [43,]   33   -9
## [44,]   43   31
## [45,]   40   38
## [46,]   -2   44
## [47,]   45   42
## [48,]   41   46
## [49,]   48   47
## Order of objects:
##  [1] Alabama        Tennessee      Georgia        Louisiana     
##  [5] Mississippi    South Carolina North Carolina Alaska        
##  [9] Arizona        Maryland       New Mexico     Michigan      
## [13] Illinois       New York       Texas          Missouri      
## [17] Florida        California     Nevada         Colorado      
## [21] Arkansas       Virginia       Wyoming        Delaware      
## [25] Indiana        Oklahoma       Ohio           Kansas        
## [29] Pennsylvania   Oregon         Washington     Connecticut   
## [33] Rhode Island   Massachusetts  New Jersey     Hawaii        
## [37] Utah           Idaho          Nebraska       Minnesota     
## [41] Wisconsin      Kentucky       Montana        Iowa          
## [45] New Hampshire  Maine          North Dakota   South Dakota  
## [49] West Virginia  Vermont       
## Height:
##  [1] 1.0340801 1.3687929 1.0408349 2.8205587 0.9771986 1.3960934 5.4623592
##  [8] 4.0837483 1.4720236 0.6743186 1.3385319 1.9655812 0.4336501 1.0044328
## [15] 1.8355779 2.9848751 3.0534495 1.4621581 1.7044355 7.4792599 1.2084105
## [22] 0.8788445 2.1852059 2.5927716 0.6281577 0.8500187 1.0553617 0.6765523
## [29] 2.1808837 0.7311305 3.7810250 1.2903008 1.9665793 0.9626545 2.5032675
## [36] 1.2698293 5.3463933 0.9132211 1.4571402 0.6215418 2.5227753 1.0181020
## [43] 2.9549994 0.2619577 0.7930730 1.2918266 2.1082699 0.8721679 1.3129132
## Divisive coefficient:
## [1] 0.8530481
## 
## Available components:
## [1] &quot;order&quot;     &quot;height&quot;    &quot;dc&quot;        &quot;merge&quot;     &quot;diss&quot;      &quot;call&quot;     
## [7] &quot;order.lab&quot; &quot;data&quot;</code></pre>
<pre class="r"><code>fviz_dend(res.diana, cex = 0.5,
          k = 4, # Cut in four groups
          palette = &quot;jco&quot; # Color palette
          )</code></pre>
<p><img src="/post/2017-10-06-Clustering_2_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
</div>
