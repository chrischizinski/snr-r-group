---
title: "Latent variable analysis. Part 1"
csl: the-journal-of-wildlife-management.csl
bibliography: bibliography.bib
date: 2017-10-13
categories: ["R"]
tags: ["Applied Multivariate"]
---



<pre class="r"><code>library(psych)</code></pre>
<div id="answers-to-the-challenge" class="section level2">
<h2>Answers to the challenge</h2>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 2.2.1     ✔ purrr   0.2.4
## ✔ tibble  1.4.2     ✔ dplyr   0.7.4
## ✔ tidyr   0.8.0     ✔ stringr 1.3.0
## ✔ readr   1.1.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ────────────────────── tidyverse_conflicts() ──
## ✖ ggplot2::%+%()      masks psych::%+%()
## ✖ ggplot2::alpha()    masks psych::alpha(), scales::alpha()
## ✖ readr::col_factor() masks scales::col_factor()
## ✖ purrr::discard()    masks scales::discard()
## ✖ dplyr::filter()     masks stats::filter()
## ✖ dplyr::lag()        masks stats::lag()</code></pre>
<pre class="r"><code>library(ggdendro)
library(NbClust)

data(&quot;USArrests&quot;)

USArrests %&gt;% 
  scale() -&gt; arrest.scale

arrest.scale %&gt;% 
  dist(method = &quot;euclidean&quot;) -&gt; arrest.euc

arrest_hclust &lt;- hclust(arrest.euc, method = &quot;ward.D2&quot;)

### Find the best number of clusters
arrest.nb &lt;- NbClust(arrest.scale, 
        distance = &quot;euclidean&quot;, 
        min.nc = 2, 
        max.nc = 10, 
        method = &quot;complete&quot;, 
        index = &quot;all&quot;)</code></pre>
<p><img src="/post/2017-10-13-LatentVariables_1_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="/post/2017-10-13-LatentVariables_1_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 9 proposed 2 as the best number of clusters 
## * 4 proposed 3 as the best number of clusters 
## * 6 proposed 4 as the best number of clusters 
## * 2 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
<pre class="r"><code>clusters &lt;- data.frame(arrest.nb$Best.partition)
clusters$state &lt;- rownames(clusters)
colnames(clusters) &lt;- c(&quot;cluster&quot;, &quot;label&quot;)

### extract the data
arrest_dendro &lt;- as.dendrogram(arrest_hclust)

### convert data to dendro data for ggplot
arrest_dendro_data &lt;- dendro_data(arrest_dendro, type = &quot;rectangle&quot;)

### combine cluster and dendro label data 
arrest_dendro_labels &lt;- data.frame(full_join(arrest_dendro_data$labels, clusters))</code></pre>
<pre><code>## Joining, by = &quot;label&quot;</code></pre>
<pre class="r"><code>arrest_dendro_segs &lt;- data.frame(arrest_dendro_data$segments) 
arrest_dendro_segs$cluster &lt;- ifelse(arrest_dendro_segs$x &lt; 20 &amp; arrest_dendro_segs$xend &lt; 20, 1, 2)

### plot 
arrest_dendro_plot&lt;-ggplot() + 
                      geom_segment(data = arrest_dendro_segs, aes(x = x, y = y, xend = xend, yend = yend, color = as.factor(cluster))) + 
                      geom_text(data = arrest_dendro_labels, aes(x = x, y = y, label = label, color = as.factor(cluster)), angle = 90, hjust = 1, size = 1.7, show.legend = FALSE) +
                      labs(color=&quot;Cluster&quot;) +  
                      coord_cartesian(ylim = c(-5,15)) +
                      theme_void() + 
                      scale_colour_manual(values = c(&quot;1&quot; = &quot;firebrick&quot;, &quot;2&quot; = &quot;dodgerblue&quot;)) +
                      theme(plot.margin = margin(0.5, 0.5, 1, 0.5, unit = &quot;cm&quot;)
                      )


arrest_dendro_plot</code></pre>
<p><img src="/post/2017-10-13-LatentVariables_1_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
</div>
<div id="a-quick-word-about-eigenvectors-and-values" class="section level2">
<h2>A quick word about eigenvectors and values</h2>
<p><a href="http://setosa.io/ev/eigenvectors-and-eigenvalues/">Eigenvectors and Eigenvalues</a></p>
<p>For any number set of points, we can digest these points into eigenvectors and eigenvalue. Eigenvectors and eigenvalues exist in pairs with eigenvector describes a direction along which a linear transformation acts simply by “stretching/compressing” and/or “flipping”; and eigenvalue describing the degree of that transformation in that direction. The numbers of eigenvectors is equal to the number of dimensions of your data.</p>
<pre class="r"><code>mat&lt;-matrix(c(3,2,1,2), byrow = TRUE, ncol = 2)

eigen(mat)</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 4 1
## 
## $vectors
##           [,1]       [,2]
## [1,] 0.8944272 -0.7071068
## [2,] 0.4472136  0.7071068</code></pre>
</div>
<div id="latent-variable-analysis" class="section level1">
<h1>Latent variable analysis</h1>
<div id="factor-analysis" class="section level2">
<h2>Factor analysis</h2>
<p>The source to many of the notes in this lesson (and a lot more detail on the subject) can be found at <span class="citation">Finch and French (2015)</span> and <span class="citation">Beaujean (2014)</span>.</p>
<p><em>Latent variables</em> in statistics are variables that are not directly observable and are inferred from a mathematical model. One advantage of using <em>latent variables</em> is that it helps reduce the dimensionality of data (a major theme of multivariate statistics) and has been used in many scientific disciplines.</p>
<p>One type of latent variable analysis is <em>factor analysis</em> and used extensively in social and behavioral sciences. Factor analysis allows the researcher to create models of non-observable factors (e.g., motivations, constraints, identity) from multivariate data.</p>
<p>There are two broad types of factor analysis: 1) Exploratory factor analysis (EFA) and 2) Confirmatory factor analysis (CFA). The difference between the two is in the degree of ** a priori ** structure that is assummed in the model. In using EFA the researcher does not impose a specific latent structure on the observable data, but allows the analysis to provide the optimal number of factors. In contrast to EFA, with CFA the researcher explicitly links the indicators with the factors to which they theoretically belong.</p>
</div>
<div id="exploratory-factor-analysis" class="section level2">
<h2>Exploratory factor analysis</h2>
<p>EFA consists of two steps (1) factor extraction and (2) factor rotation. Factor rotation involves estimating the intial model paramters (i.e., factor loadings: loadings reflect the relationships between the factors and the indicators, with larger values being indicative of a closer association between a latent and observed variable). There are as many factors as number of indicator variables (i.e., columns used to define the latent variable).</p>
<div id="factor-extraction" class="section level3">
<h3>Factor extraction</h3>
<p>Several extraction methods exist, with the most popular being maximum likelihood (ML) and principal axis factor (PAF). ML method has a direct assessment of model fit but also relies on multivariate normality. PAF does not have a distributional assumption but does not have a test of statistical fit.</p>
</div>
<div id="factor-rotation" class="section level3">
<h3>Factor rotation</h3>
<p>Factor rotation is the transformation of the initial set of factor loadings to simplify the interpretation of of the results by finding a simple solution. Methods fall into two broad categories: orthogonal and oblique. Orthogonal rotations constrain the correlations among factors to be zero, whereas oblique rotations allow the factors to be correlated. The most popular orthogonal rotation method is VARIMAX, while among the oblique rotations PROMAX and OBLIMIN are popular. Decision on which method to use, should be based in theory and empirical grounds.</p>
</div>
<div id="determining-the-optimal-number-of-factors" class="section level3">
<h3>Determining the optimal number of factors</h3>
<ol style="list-style-type: decimal">
<li>Eigenvalues greater than 1. Eigenvalues greater than 1, account for more variance than any of the observed values.<br />
</li>
<li>Scree plots. Eigenvalues on the y-axis and number of factors on the x axis. Identify where the plot “flattens out”</li>
<li>Proportion of variance. Identify when there is no “appreciative” gain in the percentage of variance explained. Similar to scree plot</li>
<li>Residual correlation matrix for the observed indicators. residuals larger than 0.05 are considered to be too large, so that a good solution is one which produces few residual correlations greater than 0.05 in absolute value.</li>
<li>Parallel analysis. Described in <span class="citation">Horn (1965)</span>. Several steps that include comparing the actual data to two randomly generated datasets. THis is the most commonly used.</li>
</ol>
</div>
<div id="the-data" class="section level3">
<h3>The data</h3>
<p>For this data, we will use the data set provided by <span class="citation">Finch and French (2015)</span> <a href="https://www.routledge.com/Latent-Variable-Modeling-with-R/Finch-French/p/book/9780415832458#datasets">here</a>. The data represents information collected on acheivement goal orientation. Achievement goal orientation refers to how an individual interprets and reacts to tasks, resulting in different patterns of cognition, affect and behavior. There are 12 questions with results representing a 7-point likert-type scale from 430 college students.</p>
<p>The columns refer to:</p>
<ul>
<li>AGS1 = My goal is to completely master the material presented in my classes. (MAP)</li>
<li>AGS2 = I want to avoid learning less than it is possible to learn. (MAV)</li>
<li>AGS3 = It is important for me to do better than other students. (PAP)</li>
<li>AGS4 = I want to avoid performing poorly compared to others. (PAV)</li>
<li>AGS5 = I want to learn as much as possible. (MAP)</li>
<li>AGS6 = It is important for me to avoid an incomplete understanding of the course material. (MAV)</li>
<li>AGS7 = It is important for me to understand the content of my courses as thoroughly as possible. (MAP)</li>
<li>AGS8 = My goal is to avoid performing worse than other students. (PAV)</li>
<li>AGS9 = I want to do well compared to other students. (PAP)</li>
<li>AGS10 = It is important for me to avoid doing poorly compared to other students. (PAV)</li>
<li>AGS11 = My goal is to perform better than the other students. (PAP)</li>
<li>AGS12 = My goal is to avoid learning less than I possibly could. (MAV)</li>
</ul>
<p>The types of questions refer to 4 distinct latent traits: mastery approach (MAP), mastery avoidant (MAV), performance approach (PAP), and performance avoidant (PAV).</p>
<p>The data is in a SPSS format and I have converted it to a csv file for convience and is in our github data repository as <code>goal_scale.csv</code></p>
<pre class="r"><code>library(readr)

goal_scale &lt;- read_csv(&quot;https://raw.githubusercontent.com/chrischizinski/SNR_R_Group/master/data/goal_scale.csv&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   ags1 = col_integer(),
##   ags2 = col_integer(),
##   ags3 = col_integer(),
##   ags4 = col_integer(),
##   ags5 = col_integer(),
##   ags6 = col_integer(),
##   ags7 = col_integer(),
##   ags8 = col_integer(),
##   ags9 = col_integer(),
##   ags10 = col_integer(),
##   ags11 = col_integer(),
##   ags12 = col_integer()
## )</code></pre>
<pre class="r"><code>head(goal_scale)</code></pre>
<pre><code>## # A tibble: 6 x 12
##    ags1  ags2  ags3  ags4  ags5  ags6  ags7  ags8  ags9 ags10 ags11 ags12
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1     6     6     6     7     7     5     7     6     6     7     6     6
## 2     5     5     6     6     7     6     5     6     6     7     5     5
## 3     5     6     2     3     7     6     7     2     4     2     2     7
## 4     5     4     7     7     6     5     4     7     7     7     7     4
## 5     5     4     5     5     6     5     5     4     5     5     3     3
## 6     7     7     7     7     7     7     7     7     7     7     7     7</code></pre>
</div>
<div id="fit-an-efa-models-with-factanal" class="section level3">
<h3>Fit an EFA models with <code>factanal</code></h3>
<pre class="r"><code>agoal.efa&lt;-factanal(~ags1+ags2+ags3+ags4+ags5+ags6+ags7+ags8+ags9+ags10+ags11+ ags12, factors=4, rotation=&quot;promax&quot;, data = goal_scale )

agoal.efa</code></pre>
<pre><code>## 
## Call:
## factanal(x = ~ags1 + ags2 + ags3 + ags4 + ags5 + ags6 + ags7 +     ags8 + ags9 + ags10 + ags11 + ags12, factors = 4, data = goal_scale,     rotation = &quot;promax&quot;)
## 
## Uniquenesses:
##  ags1  ags2  ags3  ags4  ags5  ags6  ags7  ags8  ags9 ags10 ags11 ags12 
## 0.487 0.335 0.279 0.342 0.557 0.388 0.104 0.005 0.231 0.201 0.300 0.306 
## 
## Loadings:
##       Factor1 Factor2 Factor3 Factor4
## ags1           0.667                 
## ags2                   0.844         
## ags3   0.864                         
## ags4   0.793           0.104  -0.116 
## ags5           0.565   0.123         
## ags6           0.764                 
## ags7           1.023  -0.120         
## ags8   0.756                   0.583 
## ags9   0.884                         
## ags10  0.866                   0.143 
## ags11  0.799                   0.195 
## ags12                  0.833         
## 
##                Factor1 Factor2 Factor3 Factor4
## SS loadings      4.122   2.404   1.461   0.426
## Proportion Var   0.344   0.200   0.122   0.036
## Cumulative Var   0.344   0.544   0.666   0.701
## 
## Factor Correlations:
##         Factor1 Factor2  Factor3  Factor4
## Factor1  1.0000  0.0919 -0.08477  0.20174
## Factor2  0.0919  1.0000  0.18936  0.66077
## Factor3 -0.0848  0.1894  1.00000 -0.00277
## Factor4  0.2017  0.6608 -0.00277  1.00000
## 
## Test of the hypothesis that 4 factors are sufficient.
## The chi square statistic is 77.4 on 24 degrees of freedom.
## The p-value is 0.000000157</code></pre>
<div id="uniqueness" class="section level4">
<h4>Uniqueness</h4>
<p>Uniqueness reflects the proportion of variance in the indicators that are not explained by the factors. For example, 48.7% of variation in <code>ags1</code> is not explained by the four factors.</p>
<pre class="r"><code>agoal.efa$uniquenesses</code></pre>
<pre><code>##      ags1      ags2      ags3      ags4      ags5      ags6      ags7 
## 0.4865429 0.3350171 0.2785319 0.3416042 0.5574857 0.3876667 0.1043941 
##      ags8      ags9     ags10     ags11     ags12 
## 0.0050000 0.2306985 0.2007897 0.2998883 0.3061390</code></pre>
<p>The opposite of uniqueness of communality, and this is the proportion of variances explained by the factors for each indicator.</p>
<pre class="r"><code>1-agoal.efa$uniquenesses</code></pre>
<pre><code>##      ags1      ags2      ags3      ags4      ags5      ags6      ags7 
## 0.5134571 0.6649829 0.7214681 0.6583958 0.4425143 0.6123333 0.8956059 
##      ags8      ags9     ags10     ags11     ags12 
## 0.9950000 0.7693015 0.7992103 0.7001117 0.6938610</code></pre>
</div>
<div id="loadings" class="section level4">
<h4>Loadings</h4>
<pre class="r"><code>ld&lt;-loadings(agoal.efa)
ld</code></pre>
<pre><code>## 
## Loadings:
##       Factor1 Factor2 Factor3 Factor4
## ags1           0.667                 
## ags2                   0.844         
## ags3   0.864                         
## ags4   0.793           0.104  -0.116 
## ags5           0.565   0.123         
## ags6           0.764                 
## ags7           1.023  -0.120         
## ags8   0.756                   0.583 
## ags9   0.884                         
## ags10  0.866                   0.143 
## ags11  0.799                   0.195 
## ags12                  0.833         
## 
##                Factor1 Factor2 Factor3 Factor4
## SS loadings      4.122   2.404   1.461   0.426
## Proportion Var   0.344   0.200   0.122   0.036
## Cumulative Var   0.344   0.544   0.666   0.701</code></pre>
<p>To help interpret our loadings, lets create a visualization of those loadings.</p>
<pre class="r"><code>loadings&lt;-as.data.frame(ld[,])

lt&lt;- data.frame(indicator = paste(&quot;ags&quot;,1:12, sep =&quot;&quot;),
           latent_traits = c(&quot;MAP&quot;, &quot;MAV&quot;, &quot;PAP&quot;, &quot;PAV&quot;, &quot;MAP&quot;,&quot;MAV&quot;, &quot;MAP&quot;, &quot;PAV&quot;, &quot;PAP&quot;, &quot;PAV&quot;, &quot;PAP&quot;, &quot;MAV&quot;))

loadings %&gt;% 
  rownames_to_column(&quot;indicator&quot;) %&gt;% 
  left_join(lt) %&gt;% 
  mutate(indicator = factor(indicator, levels = paste(&quot;ags&quot;,12:1, sep =&quot;&quot;))) %&gt;% 
  gather(factor, value, -indicator, - latent_traits) %&gt;% 
  mutate(value2 = ifelse(value &lt; 0.1, NA,  value))-&gt; loadings.long </code></pre>
<pre><code>## Joining, by = &quot;indicator&quot;</code></pre>
<pre class="r"><code>ggplot(data = loadings.long) +
  geom_point(aes(x = factor, y = indicator, color = value2, shape = latent_traits), size = 8) +
  scale_colour_gradient(na.value = &quot;white&quot;, low = &quot;blue&quot;, high = &quot;red&quot;) +
  scale_shape_manual(values = c(&quot;MAP&quot; = 15, &quot;MAV&quot; = 16, &quot;PAP&quot; = 17, &quot;PAV&quot; = 18)) +
  labs(color = &quot;Loading&quot;, shape = &quot;Latent\ntrait&quot;) +
  theme_classic()</code></pre>
<p><img src="/post/2017-10-13-LatentVariables_1_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-beaujean2014latent">
<p>Beaujean, A. A. 2014. Latent variable modeling using r: A step-by-step guide. Routledge.</p>
</div>
<div id="ref-finch2015latent">
<p>Finch, W. H., and B. F. French. 2015. Latent variable modeling with r. Routledge.</p>
</div>
<div id="ref-horn1965rationale">
<p>Horn, J. L. 1965. A rationale and test for the number of factors in factor analysis. Psychometrika 30:179–185. Springer.</p>
</div>
</div>
</div>
</div>
